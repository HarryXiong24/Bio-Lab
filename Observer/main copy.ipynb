{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.9/Project.toml`\n",
      "  \u001b[90m[de0858da] \u001b[39m\u001b[92m+ Printf\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "\n",
    "# Pkg.add(\"NNlib\")\n",
    "# Pkg.add(\"CUDA\")\n",
    "# Pkg.add(\"DICOM\")\n",
    "# Pkg.add(url=\"https://github.com/MolloiLab/imageToolBox.jl\")\n",
    "# Pkg.add(\"Lux\")\n",
    "# Pkg.add(\"LuxCUDA\")\n",
    "# Pkg.add(\"NNlib\")\n",
    "# Pkg.add(\"NIfTI\")\n",
    "# Pkg.add(\"Images\")\n",
    "# Pkg.add(\"ImageFiltering\")\n",
    "# Pkg.add(\"Statistics\")\n",
    "# Pkg.add(\"ImageMorphology\")\n",
    "# Pkg.add(\"CSV\")\n",
    "# Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Printf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "# add the following packages before running\n",
    "using Lux, NNlib, LuxCUDA, CUDA, JLD2, NIfTI, DICOM\n",
    "using Images, imageToolBox, ImageFiltering, Statistics\n",
    "using ImageMorphology\n",
    "using CSV, DataFrames\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "    l1 = Chain(\n",
       "        layer_1 = Conv((3, 3), 1 => 16, pad=1),  \u001b[90m# 160 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 16 => 16, pad=1),  \u001b[90m# 2_320 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "    ),\n",
       "    l2 = Chain(\n",
       "        layer_1 = MaxPool((2, 2)),\n",
       "        layer_2 = Conv((3, 3), 16 => 32, pad=1),  \u001b[90m# 4_640 parameters\u001b[39m\n",
       "        layer_3 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "        layer_4 = Conv((3, 3), 32 => 32, pad=1),  \u001b[90m# 9_248 parameters\u001b[39m\n",
       "        layer_5 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "    ),\n",
       "    l3 = Chain(\n",
       "        layer_1 = MaxPool((2, 2)),\n",
       "        layer_2 = Conv((3, 3), 32 => 64, pad=1),  \u001b[90m# 18_496 parameters\u001b[39m\n",
       "        layer_3 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "        layer_4 = Conv((3, 3), 64 => 64, pad=1),  \u001b[90m# 36_928 parameters\u001b[39m\n",
       "        layer_5 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "    ),\n",
       "    l4 = Chain(\n",
       "        layer_1 = MaxPool((2, 2)),\n",
       "        layer_2 = Conv((3, 3), 64 => 128, pad=1),  \u001b[90m# 73_856 parameters\u001b[39m\n",
       "        layer_3 = BatchNorm(128, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 257\u001b[39m\n",
       "        layer_4 = Conv((3, 3), 128 => 128, pad=1),  \u001b[90m# 147_584 parameters\u001b[39m\n",
       "        layer_5 = BatchNorm(128, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 257\u001b[39m\n",
       "        layer_6 = ConvTranspose((2, 2), 128 => 64, stride=2),  \u001b[90m# 32_832 parameters\u001b[39m\n",
       "        layer_7 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "    ),\n",
       "    l5 = Chain(\n",
       "        layer_1 = Conv((3, 3), 128 => 64, pad=1),  \u001b[90m# 73_792 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 64 => 64, pad=1),  \u001b[90m# 36_928 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "        layer_5 = ConvTranspose((2, 2), 64 => 32, stride=2),  \u001b[90m# 8_224 parameters\u001b[39m\n",
       "        layer_6 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "    ),\n",
       "    l6 = Chain(\n",
       "        layer_1 = Conv((3, 3), 64 => 32, pad=1),  \u001b[90m# 18_464 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 32 => 32, pad=1),  \u001b[90m# 9_248 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "        layer_5 = ConvTranspose((2, 2), 32 => 16, stride=2),  \u001b[90m# 2_064 parameters\u001b[39m\n",
       "        layer_6 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "    ),\n",
       "    l7 = Chain(\n",
       "        layer_1 = Conv((3, 3), 32 => 16, pad=1),  \u001b[90m# 4_624 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 16 => 16, pad=1),  \u001b[90m# 2_320 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "        layer_5 = Conv((1, 1), 16 => 1),  \u001b[90m# 17 parameters\u001b[39m\n",
       "        layer_6 = WrappedFunction(Ïƒ),\n",
       "    ),\n",
       ") \u001b[90m        # Total: \u001b[39m483_377 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m1_649 states."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The U-net model\n",
    "\n",
    "_conv = (in, out) -> Conv((3, 3), in => out, pad=1)\n",
    "conv1 = (in, out) -> Chain(_conv(in, out), BatchNorm(out, leakyrelu))\n",
    "\n",
    "_tran = (in, out) -> ConvTranspose((2, 2), in => out, stride=2)\n",
    "tran = (in, out) -> Chain(_tran(in, out), BatchNorm(out, leakyrelu))\n",
    "\n",
    "struct UNet{\n",
    "    CH1,CH2,CH3,CH4,CH5,CH6,CH7\n",
    "} <: Lux.AbstractExplicitContainerLayer{\n",
    "    (:l1, :l2, :l3, :l4, :l5, :l6, :l7)\n",
    "}\n",
    "    l1::CH1\n",
    "    l2::CH2\n",
    "    l3::CH3\n",
    "    l4::CH4\n",
    "    l5::CH5\n",
    "    l6::CH6\n",
    "    l7::CH7\n",
    "end\n",
    "\n",
    "function UNet(in_chs, lbl_chs, size)\n",
    "    l1 = Chain(conv1(in_chs, size), conv1(size, size))\n",
    "    l2 = Chain(MaxPool((2, 2), stride=2), conv1(size, size * 2), conv1(size * 2, size * 2))\n",
    "    l3 = Chain(MaxPool((2, 2), stride=2), conv1(size * 2, size * 4), conv1(size * 4, size * 4))\n",
    "    l4 = Chain(MaxPool((2, 2), stride=2), conv1(size * 4, size * 8), conv1(size * 8, size * 8), tran(size * 8, size * 4))\n",
    "\n",
    "    # Expanding layers\n",
    "    l5 = Chain(conv1(size * 8, size * 4), conv1(size * 4, size * 4), tran(size * 4, size * 2))\n",
    "    l6 = Chain(conv1(size * 4, size * 2), conv1(size * 2, size * 2), tran(size * 2, size))\n",
    "    l7 = Chain(conv1(size * 2, size), conv1(size, size), Conv((1, 1), size => lbl_chs), sigmoid)\n",
    "\n",
    "    UNet(l1, l2, l3, l4, l5, l6, l7)\n",
    "end\n",
    "\n",
    "function (m::UNet)(x, ps, st::NamedTuple)\n",
    "    # Convolutional layers\n",
    "    x1, st_l1 = m.l1(x, ps.l1, st.l1)\n",
    "\n",
    "    x2, st_l2 = m.l2(x1, ps.l2, st.l2)\n",
    "\n",
    "    # Downscaling Blocks\n",
    "    x3, st_l3 = m.l3(x2, ps.l3, st.l3)\n",
    "    x4, st_l4 = m.l4(x3, ps.l4, st.l4)\n",
    "\n",
    "    # Upscaling Blocks\n",
    "    x5, st_l5 = m.l5(cat(x4, x3; dims=3), ps.l5, st.l5)\n",
    "    x6, st_l6 = m.l6(cat(x5, x2; dims=3), ps.l6, st.l6)\n",
    "    x7, st_l7 = m.l7(cat(x6, x1; dims=3), ps.l7, st.l7)\n",
    "\n",
    "\n",
    "    # Merge states\n",
    "    st = (\n",
    "        l1=st_l1, l2=st_l2, l3=st_l3, l4=st_l4, l5=st_l5, l6=st_l6, l7=st_l7\n",
    "    )\n",
    "\n",
    "    return x7, st\n",
    "end\n",
    "\n",
    "model_to_use = UNet(1, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resize_dicom_image (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function crop_to_bounding_box(mask, image, ground_truth_mask)\n",
    "    # Find rows and columns where the mask has value 1\n",
    "    indices = findall(x -> x == 1, mask)\n",
    "    if isempty(indices)\n",
    "        return \"Error in breast mask!\" # Return the original mask if no 1s are found\n",
    "    end\n",
    "\n",
    "    # Extract row and column indices from the CartesianIndex array\n",
    "    rows = [index[1] for index in indices]\n",
    "    cols = [index[2] for index in indices]\n",
    "\n",
    "    # Determine the initial bounding box\n",
    "    min_row, max_row = minimum(rows), maximum(rows)\n",
    "    min_col, max_col = minimum(cols), maximum(cols)\n",
    "\n",
    "    # Adjust dimensions to be divisible by 32\n",
    "    width, height = max_col - min_col + 1, max_row - min_row + 1\n",
    "    if width % 32 != 0\n",
    "        width_adjustment = 32 - (width % 32)\n",
    "        max_col += width_adjustment\n",
    "        # Move the bounding box if it exceeds mask dimensions\n",
    "        if max_col > size(mask, 2)\n",
    "            min_col = max(1, min_col - (max_col - size(mask, 2)))\n",
    "            max_col = size(mask, 2)\n",
    "        end\n",
    "    end\n",
    "    if height % 32 != 0\n",
    "        height_adjustment = 32 - (height % 32)\n",
    "        max_row += height_adjustment\n",
    "        # Move the bounding box if it exceeds mask dimensions\n",
    "        if max_row > size(mask, 1)\n",
    "            min_row = max(1, min_row - (max_row - size(mask, 1)))\n",
    "            max_row = size(mask, 1)\n",
    "        end\n",
    "    end\n",
    "    rslt = image[min_row:max_row, min_col:max_col]\n",
    "    rslt2 = ground_truth_mask[min_row:max_row, min_col:max_col]\n",
    "    rslt3 = [min_row, max_row, min_col, max_col]\n",
    "\n",
    "    x, y = size(rslt)\n",
    "    res = x % 32\n",
    "    if res != 0\n",
    "        top_trim = round(Int, res / 2)\n",
    "        bot_trim = res - top_trim\n",
    "        rslt = rslt[top_trim+1:end-bot_trim, 1:end]\n",
    "        rslt2 = rslt2[top_trim+1:end-bot_trim, 1:end]\n",
    "        rslt3[1] += top_trim\n",
    "        rslt3[2] -= bot_trim\n",
    "    end\n",
    "\n",
    "    res = y % 32\n",
    "    if res != 0\n",
    "        left_trim = round(Int, res / 2)\n",
    "        right_trim = res - left_trim\n",
    "        rslt = rslt[1:end, left_trim+1:end-right_trim]\n",
    "        rslt2 = rslt2[1:end, left_trim+1:end-right_trim]\n",
    "        rslt3[3] += left_trim\n",
    "        rslt3[4] -= right_trim\n",
    "    end\n",
    "\n",
    "    return rslt, rslt2, rslt3\n",
    "end\n",
    "\n",
    "function resize_dicom_image(image, mask, ground_truth_mask, original_spacing; target_spacing=[0.13, 0.13])\n",
    "    # Calculate the scaling factors\n",
    "    scale_x = original_spacing[1] / target_spacing[1]\n",
    "    scale_y = original_spacing[2] / target_spacing[2]\n",
    "\n",
    "    new_size_x = round(Int, size(image, 1) * scale_y)\n",
    "    new_size_y = round(Int, size(image, 2) * scale_x)\n",
    "    # Resample the image\n",
    "    resized_img = imresize(image, (new_size_x, new_size_y))\n",
    "    resized_mask = imresize(mask, (new_size_x, new_size_y))\n",
    "    resized_ground_truth_mask = imresize(ground_truth_mask, (new_size_x, new_size_y))\n",
    "    return resized_img, resized_mask, resized_ground_truth_mask, [new_size_x, new_size_y]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â”Œ Warning: LuxCUDA is loaded but the CUDA GPU is not functional.\n",
      "â”” @ LuxCUDA /Users/harryxiong24/.julia/packages/LuxCUDA/QvUoj/src/LuxCUDA.jl:20\n",
      "â”Œ Warning: No functional GPU backend found! Defaulting to CPU.\n",
      "â”‚ \n",
      "â”‚ 1. If no GPU is available, nothing needs to be done.\n",
      "â”‚ 2. If GPU is available, load the corresponding trigger package.\n",
      "â”‚     a. LuxCUDA.jl for NVIDIA CUDA Support!\n",
      "â”‚     b. LuxAMDGPU.jl for AMD GPU ROCM Support!\n",
      "â”‚     c. Metal.jl for Apple Metal GPU Support!\n",
      "â”” @ LuxDeviceUtils /Users/harryxiong24/.julia/packages/LuxDeviceUtils/eyk2C/src/LuxDeviceUtils.jl:154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(::LuxCPUDevice) (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_saved_model = \"saved_train_info_112.jld2\"\n",
    "dev = gpu_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((l1 = (layer_1 = (weight = [-0.13811347 -0.10989219 -0.45017767; -0.03557757 0.15679145 -0.10025815; 0.4154285 0.11406401 -0.1702349;;;; -0.2519165 -0.2005901 0.269674; 0.21714726 0.121530145 0.1745333; 0.32218948 -0.15850435 -0.16568863;;;; 0.05944323 -0.19018307 0.27134103; 0.1708762 -0.094147116 0.019200183; 0.12004882 -0.29905564 -0.2584902;;;; â€¦ ;;;; 0.34197986 -0.2045819 -0.03704661; 0.17001131 -0.23942131 -0.083972864; 0.118663475 0.054883588 0.13577364;;;; 0.21762453 -0.1118695 -0.12177359; -0.29748553 0.5254144 0.24555175; -0.0060818857 -0.16924761 0.020385385;;;; 0.05229594 0.20575164 -0.1876456; 0.031438395 -0.21704458 -0.39874387; 0.33417702 -0.10161231 -0.0084489], bias = [0.44849735;;; 0.11582101;;; -0.3342599;;; â€¦ ;;; 0.10559988;;; 0.6723078;;; -0.23398082;;;;]), layer_2 = (scale = Float32[0.41381815, 0.38657746, 0.48907733, 0.97282636, 1.3967147, 0.3998129, 0.5293896, 0.43151233, 1.6391323, 0.60299855, 0.466895, 0.37334907, 0.70797276, 0.3011922, 1.3915981, 0.7733621], bias = Float32[-0.24501269, -0.73498076, -0.16275617, 0.064734064, 0.4630046, -0.5492776, -0.44436267, -0.5949571, 0.52130777, -0.06491986, -0.28573677, -0.2430272, -0.4310258, -0.5599328, -0.13903986, -0.14869845]), layer_3 = (weight = [0.13761187 0.07237095 0.12714948; 0.003942846 -0.16441748 0.15846805; 0.11252243 -0.19839178 0.11318704;;; -0.09781886 -0.028421417 -0.023553248; -0.09453351 -0.13242482 -0.044346444; -0.08160121 -0.3425312 -0.22185655;;; -0.11518222 0.07433038 0.13413583; 0.08342823 0.10712396 -0.0897541; 0.035951965 -0.09806621 -0.07589495;;; â€¦ ;;; 0.10583228 0.06410672 -0.10994717; -0.1633928 0.09846377 0.025138494; -0.116282016 -0.10635675 -0.0005288702;;; -0.09128721 0.29818454 0.47792435; 0.19948597 -0.10995611 -0.26877335; -0.18034059 0.011199035 -0.0014523555;;; -0.029298635 -0.22506768 -0.106658265; -0.048513662 -0.118061624 0.23961551; -0.0964448 -0.053690568 0.17131148;;;; 0.16553795 -0.124168955 -0.13870199; -0.1435295 -0.047067795 -0.005438062; 0.041886993 -0.33593422 -0.0031135886;;; 0.012736593 0.053857237 0.1469941; 0.30626497 0.13723493 0.063509695; 0.08275644 -0.12377626 -0.015864067;;; 0.050026707 0.22877039 0.20372732; -0.061993323 0.20573926 0.19763917; -0.088048294 0.007501012 0.21301559;;; â€¦ ;;; 0.100061126 -0.045535654 0.19283947; 0.18640128 0.1133292 0.1700074; 0.3156568 0.29396537 0.3146615;;; -0.066182695 -0.26318094 0.22582665; 0.3167046 -0.19096914 0.21010955; -0.20772596 0.12509382 -0.05871395;;; -0.012221089 -0.038946643 0.006924226; 0.042293847 0.06166178 -0.044456866; 0.48451203 0.0737782 0.2996542;;;; 0.122096844 0.007365815 -0.14168638; 0.3093178 0.16106008 -0.050250333; 0.13523433 0.17346251 -0.015018543;;; 0.10640456 -0.06377388 -0.08692791; -0.27705008 0.21674249 0.026522126; -0.3663557 0.030517204 -0.04315807;;; 0.2688078 0.037831448 -0.0013327006; -0.2877358 -0.07997895 -0.11530796; -0.078744784 -0.16770874 -0.18951745;;; â€¦ ;;; 0.082176715 0.0015754777 0.14318636; 0.26852033 0.063158885 -0.15351254; 0.017506037 0.060388092 -0.21883541;;; -0.3001915 -0.09028647 -0.11405308; -0.14211956 -0.14597085 -0.25886387; 0.35192773 0.10343872 -0.16161005;;; 0.41790184 0.23921357 0.06504134; 0.2658841 -0.02080806 -0.095854945; -0.06543479 -0.18716381 -0.023204608;;;; â€¦ ;;;; -0.19139689 -0.07589573 0.14088845; -0.0886444 -0.16234174 0.24975778; -0.049569376 -0.25964907 -0.19475625;;; -0.11278073 -0.33063123 -0.22528994; -0.2029897 -0.2044259 0.1788333; -0.01453759 -0.13782457 -0.23434703;;; -0.14509639 -0.36499923 -0.34808272; -0.021478023 -0.006242068 -0.08089344; -0.024693383 0.30051324 0.14251937;;; â€¦ ;;; 0.04430369 0.26190776 -0.041252173; -0.050177097 0.15219998 0.044619802; -0.0810776 0.09083306 0.03774417;;; -0.2484032 0.3489411 0.2572406; 0.23410621 0.29285622 0.96181697; 0.09046777 -0.042855393 0.21261007;;; -0.09686711 -0.021604188 -0.1137666; 0.0593329 -0.19911484 -0.3984984; -0.23186396 -0.27337635 0.12670487;;;; -0.25045693 -0.23277332 -0.21813236; 0.014237588 -0.2462513 -0.30063355; 0.17588495 0.23662046 0.043968756;;; -0.030446257 -0.018354546 -0.15078881; -0.13679741 -0.0931066 0.0058818767; -0.0571174 0.17010735 0.030524857;;; -0.123009175 0.086552516 -0.022725405; -0.10284002 0.07434677 -0.18297522; 0.057635248 0.13425188 0.2185114;;; â€¦ ;;; -0.11236687 0.07875016 -0.16396497; -0.008945272 -0.013057039 0.12420455; 0.0059059775 -0.040036477 0.032448664;;; -0.07231179 0.13743098 0.175887; -0.33623958 -0.24589148 0.0031469094; -0.015380838 0.11756163 0.11459306;;; 0.05837692 -0.15501875 -0.20853944; 0.11619786 -0.09488888 -0.055087436; 0.11361814 0.034455467 -0.05935077;;;; 0.07461459 0.12466395 -0.049684405; 0.06770935 0.24794483 -0.028327752; 0.06548932 0.22096567 0.082554705;;; 0.6786972 0.6718689 0.5389302; 0.52819544 0.70592433 0.69460875; 0.6515936 0.7351221 0.7081719;;; 0.1080519 -0.025124773 0.0032665234; 0.30372822 0.34857598 0.22111812; 0.094014 0.119543016 0.14144436;;; â€¦ ;;; 0.013253753 0.15358625 -0.029774873; 0.13523035 0.07129381 0.05112313; 0.15627101 0.23718911 0.34266213;;; 0.0071646003 0.17203414 -0.18467095; -0.1714502 -0.27231017 0.12703802; -0.21594675 -0.32238263 -0.23465446;;; 0.07372063 0.119864725 -0.091512434; 0.028605565 0.28791064 -0.0025807444; 0.34261253 0.59410465 0.270795], bias = [-0.34894794;;; 0.16896419;;; -0.17464645;;; â€¦ ;;; -0.2531462;;; 0.05869117;;; -0.08453487;;;;]), layer_4 = (scale = Float32[1.1110313, 0.5189689, 0.423054, 0.52489763, 0.42210123, 0.59275305, 0.5040271, 0.712904, 1.5635741, 0.5072846, 0.963708, 0.65385777, 0.99238545, 1.6596597, 0.407981, 0.53248113], bias = Float32[-0.3214028, -0.20607747, -0.8468051, -0.52067363, -0.26051706, -0.29597312, -0.6365767, -0.36223224, 0.24444021, -0.05303336, -0.34777945, -0.76218927, 0.26814, 0.5030006, -0.54948384, -0.58674294])), l2 = (layer_1 = NamedTuple(), layer_2 = (weight = [-0.073387496 -0.057977833 -0.09988852; -0.01535793 0.26038054 0.21288021; 0.036678866 0.30638918 0.13513914;;; -0.014381017 -0.03407554 -0.021210099; 0.22532049 0.6209184 0.45942876; 0.09507182 0.35025454 0.25059363;;; -0.34467864 -0.31264794 -0.29863113; -0.2799256 -0.06832991 -0.23063648; -0.25617385 0.18225995 0.28316814;;; â€¦ ;;; -0.045112606 0.033715233 -0.028762493; 0.03621999 0.27932882 0.17013195; -0.17698768 0.16350605 0.14149427;;; 0.13475664 0.33100268 0.17256409; -0.036602374 0.32195777 0.16742241; -0.079948016 0.36709392 0.33881634;;; -0.19955619 0.14139068 -0.14204769; 0.07501165 0.71761745 0.19273506; 0.022910697 0.6317641 0.34434724;;;; 0.045336988 0.22256982 -0.084735714; -0.012190693 -0.4352482 0.05167743; -0.20020819 -0.118276365 0.53445816;;; -0.19195811 -0.13849495 -0.20849141; -0.27764216 0.0018549514 0.023906566; -0.2618888 -0.20476009 -0.29100442;;; -0.18445162 -0.06714669 -0.34958005; 0.061646365 0.04402576 -0.37216514; 0.005453121 -0.23718245 -0.451462;;; â€¦ ;;; 0.22844248 0.07102497 0.08829454; 0.14484572 0.07095004 0.10011539; -0.45539933 -0.3823768 0.01165146;;; -0.16431011 -0.11881519 0.027960544; -0.03264423 0.18170586 0.1832871; -0.12938064 -0.109992124 -0.0683984;;; 0.36404452 0.29862052 -0.15230875; 0.12041835 -0.07064011 -0.2301331; 0.09511337 -0.037775487 -0.123976626;;;; -0.14967766 0.02654197 0.26387817; 0.05265336 -0.19558455 -0.009216557; -0.07563949 -0.106470644 -0.20705257;;; -0.042875517 0.03912556 0.010258733; 0.03158565 0.025775641 0.02265554; -0.10236819 -0.36687535 0.26652503;;; -0.1629198 -0.28844604 0.033527765; -0.09279002 -0.119908266 -0.11500995; -0.08432936 -0.3688011 -0.08984254;;; â€¦ ;;; 0.10531986 0.056206465 -0.043316934; -0.085423544 -0.08524824 -0.19640332; -0.19483109 0.7975718 -0.29252648;;; 0.10745815 -0.110663936 0.2559639; -0.097341634 -0.2762939 0.03648664; 0.028345704 -0.18164016 0.035394978;;; 0.194399 0.27733347 -0.13076648; 0.1744503 0.16159222 -0.11574981; -0.13239974 0.0706777 -0.32571223;;;; â€¦ ;;;; -0.30073068 -0.0005078957 -0.24285394; -0.2967546 -0.18406022 -0.3742901; -0.2756956 -0.33294556 -0.4213701;;; -0.13748927 -0.16837989 -0.10031867; -0.3119768 -0.21706066 -0.22175299; -0.21977669 -0.2402609 -0.09471713;;; -0.004367392 -0.06826785 0.0060738795; 0.048354182 0.038458914 0.050501715; -0.20586897 -0.1685433 -0.12366984;;; â€¦ ;;; -0.14330125 0.111359805 0.008622767; -0.106628105 0.02840252 -0.16638708; 0.11593823 -0.023287592 -0.1996838;;; -0.24779108 -0.12406275 -0.17620082; -0.27020976 -0.16570139 -0.005834666; -0.17224295 -0.11416651 -0.22654374;;; -0.36748627 -0.2202256 -0.27319497; -0.36516213 -0.255019 -0.28382888; -0.53370094 -0.21207148 -0.31698245;;;; 0.21960916 -0.17028806 -0.13754763; -0.07540481 0.25551116 -0.19833243; 0.38651413 0.035342358 -0.22775011;;; 0.1032043 -0.02258139 -0.038871106; -0.13393845 -0.09747296 0.17268534; 0.011945038 -0.0668038 -0.17104481;;; -0.034910236 -0.1361076 -0.06623477; -0.060109418 -0.12220599 0.009353527; -0.31634262 -0.04027918 -0.061137415;;; â€¦ ;;; 0.031319216 0.17159538 -0.27300733; 0.6063962 0.6009949 -0.45689848; -0.09645516 -0.1956227 -0.20008174;;; -0.09674944 -0.07058112 -0.026242109; -0.03351462 -0.13158558 0.10406804; 0.14687513 -0.023973893 0.028386973;;; 0.0016288032 0.046209056 -0.1935407; -0.16384827 0.09983365 -0.29365575; 0.02733777 0.03632505 -0.0930211;;;; -0.12251364 -0.4349292 -0.034579862; -0.049424484 -0.13046822 -0.023730319; -0.025940098 -0.24294917 -0.08687777;;; -0.44219723 -0.30747762 -0.12822893; -0.26050016 -0.049814314 0.18503968; -0.14234085 0.14218953 0.117447264;;; 0.10884197 0.054164134 0.18804868; -0.23113142 0.005327104 0.07092795; -0.1093228 0.0013179285 -0.04646646;;; â€¦ ;;; -0.067875564 -0.1307438 -0.15896752; -0.06138425 -0.047689468 -0.23758915; 0.12579773 0.03623582 0.23951986;;; -0.39484018 -0.21098188 -0.12932609; -0.3971749 -0.18342318 -0.17194657; 0.05159597 -0.059204474 0.13717933;;; -0.42746603 -0.5565145 -0.42044604; -0.3649266 -0.297062 -0.21089251; -0.29041213 -0.0368992 -0.15951289], bias = [0.0041789347;;; 0.01007129;;; 0.13361175;;; â€¦ ;;; 0.012878014;;; 0.055823;;; -0.01464089;;;;]), layer_3 = (scale = Float32[0.6179507, 1.2437776, 1.4697515, 1.199929, 1.2292908, 1.3083096, 1.1996652, 1.3914322, 0.6478677, 0.7708837  â€¦  1.577766, 0.82139415, 1.2473419, 0.81838846, 0.59222007, 1.2175788, 1.0834994, 0.6133446, 1.664879, 0.7353449], bias = Float32[-0.42902175, -0.19429201, 0.19214238, -0.17759553, -0.20515278, -0.063003674, -0.11800948, -0.14724618, -0.5880503, -0.23331429  â€¦  -0.041846108, 0.122120924, -0.05755919, -0.36405358, -0.6042061, -0.20282269, -0.08386922, -0.13617587, 0.011180247, -0.34698692]), layer_4 = (weight = [-0.24632926 -0.28417706 -0.20062529; -0.11587239 0.009032168 0.0001817956; 0.005412696 0.011264146 0.04034398;;; 0.04457354 0.26747546 0.037006997; 0.24830431 0.075314306 -0.14698623; 0.6608268 0.15054205 -0.332632;;; -0.041374892 -0.18057019 -0.24216014; -0.015734486 0.16744916 -0.1745245; -0.13018638 0.08942005 0.029365085;;; â€¦ ;;; 0.04389622 0.03738004 0.049444363; 0.20047405 0.18200131 0.16922171; -0.08949592 0.035642665 0.11057472;;; 0.12234875 -0.1975121 -0.26020306; 0.0952502 -0.527314 -0.6054634; 0.20307639 -0.083129235 -0.41980198;;; 0.2126183 0.33565903 0.012748328; 0.16255318 0.09599111 -0.07181075; 0.53026533 0.10812314 -0.14495312;;;; 0.26143774 0.34739095 0.47958973; 0.1659965 0.097713925 -0.09126869; 0.020125497 0.015113726 -0.21348491;;; -0.03784527 -0.08494723 -0.1876464; -0.030091964 -0.009072618 -0.043796513; -0.040959835 -0.10977065 -0.18385838;;; 0.047725823 -0.05050035 -0.036578886; -0.17511514 -0.14410666 -0.0889832; -0.13206081 -0.100606024 0.04171663;;; â€¦ ;;; -0.26702654 -0.35065252 -0.24043287; 0.16095695 -0.032723863 -0.10492511; 0.29800537 0.13254419 -0.07412985;;; -0.08106669 -0.14814281 -0.2054278; -0.0453228 -0.025780357 -0.06838157; 0.30641618 0.12691264 0.00028881637;;; 0.2166745 0.024333667 -0.10875696; 0.087772556 -0.06925626 -0.1085995; 0.16374901 0.04192221 -0.09153309;;;; 0.24901904 0.11732956 0.096306115; 0.13314064 0.12330717 0.06239221; 0.20542085 0.25835857 0.23690309;;; -0.14541087 -0.031692877 -0.109502904; -0.07225533 0.40074763 0.11270933; -0.030675452 0.12636901 -0.03582059;;; 0.034450725 0.14677666 0.016055536; -0.025031354 0.028563403 0.06794033; -0.12167563 0.036835875 0.06301051;;; â€¦ ;;; 0.12725754 0.17276469 0.15224178; -0.015524475 -0.0055365055 0.19796298; 0.021514455 0.14580777 0.2189132;;; -0.066595 0.045826327 -0.16363126; -0.04597533 0.16514006 0.22780216; -0.073690414 0.17982231 0.22405992;;; 0.057996165 -0.07483413 -0.13143581; -0.1674504 -0.06827605 -0.21437132; -0.2487478 -0.278025 -0.29652908;;;; â€¦ ;;;; 0.14023854 0.019247202 -0.14387801; 0.0778775 0.036693215 -0.14555554; -0.01400145 0.10034691 0.005365585;;; 0.27859178 0.051374603 -0.17166011; 0.043336593 0.058302484 -0.033472512; 0.062256854 -0.30472827 -0.36288434;;; -0.34459534 -0.36298642 0.07852062; 0.0828684 -0.18998626 0.25733265; 0.051463783 0.15524358 0.08001127;;; â€¦ ;;; 0.09164892 0.055297304 -0.05568186; 0.06963764 0.046833675 0.12804237; 0.13332021 0.26759905 0.42164186;;; -0.23724471 -0.029063731 0.2625014; -0.52700317 -0.504307 0.0905906; -0.17107879 -0.38334957 0.08897017;;; -0.23932305 0.13412319 -0.29096404; 0.026212491 0.042779427 -0.024885708; 0.018356627 -0.5984075 -0.3879966;;;; 0.12045891 0.7699747 0.84373647; -0.03924952 0.4611262 0.36497623; -0.12183387 0.13224944 0.2103689;;; 0.21431649 0.14584993 -0.10479793; 0.20141795 0.12450574 0.07298814; -0.023115192 0.02238477 -0.07061827;;; 0.0020140545 -0.0488637 -0.00021937092; -0.19206537 -0.31584755 -0.03281554; -0.24533756 -0.42993456 0.013021797;;; â€¦ ;;; 0.04217839 -0.21166374 -0.03709773; 0.03792178 -0.065379836 -0.06421074; 0.3399908 -0.042834874 -0.030816283;;; -0.22294383 -0.00043065404 0.043648943; -0.04969151 -0.08995302 0.20603721; -0.08678143 -0.24806607 -0.2050701;;; 0.3847184 0.28607845 0.17250335; 0.25023818 -0.14811127 -0.27373797; 0.11726898 0.048528418 -0.30436102;;;; -0.079462826 -0.025773173 0.053266644; -0.10413592 -0.19837902 -0.11184987; -0.14270715 -0.026254233 -0.12334231;;; 0.23970853 0.63036007 0.110284366; -0.054439917 -0.29286647 -0.56324583; -0.5491788 -0.11395724 -0.2883523;;; -0.09247266 0.09770161 0.06600032; 0.064966165 0.17356183 -0.15237604; 0.11201858 -0.13080144 0.09879932;;; â€¦ ;;; 0.060145993 0.32240775 0.10398388; 0.1433544 0.028426033 0.16248813; 0.030496836 0.13227749 0.23194529;;; 0.118411176 -0.090720356 -0.10794772; 0.037513558 0.06806319 0.14444567; 0.015851203 0.115829766 -0.04883725;;; 0.1564358 0.15844832 0.25964576; -0.00068553543 -0.09656447 -0.41027686; 0.028641025 -0.0036033134 -0.12909485], bias = [0.055884995;;; 0.0015051824;;; -0.003758317;;; â€¦ ;;; 0.070194036;;; -0.00058140786;;; 0.067916155;;;;]), layer_5 = (scale = Float32[0.92323554, 0.52058583, 0.7038446, 0.74567926, 1.2662369, 1.3825485, 0.5485169, 0.48820275, 0.61682856, 1.1409618  â€¦  0.58958995, 0.87365603, 1.1948745, 0.43442273, 0.57825077, 0.34706274, 0.36428997, 1.1357888, 0.42703766, 0.95740503], bias = Float32[-0.27468604, -0.4795702, -0.49028412, -0.112595305, -0.33597392, -0.26906213, -0.50268126, -0.32288775, -0.1872325, -0.3966761  â€¦  -0.32133242, -0.23190486, -0.17344306, -0.47289422, -0.42085248, -0.5514277, -0.3423355, 0.001655337, -0.3111684, -0.08166178])), l3 = (layer_1 = NamedTuple(), layer_2 = (weight = [0.23166 -0.055388663 0.040390026; -0.34287384 0.16718094 0.23026665; 0.06822878 0.033686142 -0.16007556;;; -0.1373005 -0.09720722 -0.0957315; -0.12725201 -0.1602483 -0.0012648888; -0.17306033 -0.08372953 -0.029549025;;; -0.06632189 0.03850036 -0.044009525; 0.08779391 0.07449552 0.05835447; 0.08586237 -0.0012815181 0.058877975;;; â€¦ ;;; -0.0025916344 0.1037809 -0.19928946; -0.36570254 -0.22196355 0.2569677; 0.20581551 0.30278307 -0.1949427;;; -0.16578236 0.03201823 -0.006642558; -0.13492098 -0.15979953 0.07609558; -0.10003197 -0.31472 -0.007477635;;; 0.22165477 -0.11333386 -0.19195357; -0.11762399 -0.42882574 0.32956707; -0.08816863 -0.27794683 0.35895884;;;; 0.010322073 0.03402239 0.222433; -0.4605727 -0.063122556 0.071178; -0.35481828 -0.19579749 0.04005808;;; -0.037295636 -0.276963 -0.22133109; 0.014317624 0.09547163 0.052892666; 0.17547257 0.18275934 0.19278733;;; -0.49664545 -0.40468177 -0.15721881; -0.10310371 -0.2347033 0.094856165; -0.14288369 -0.08055482 0.06358545;;; â€¦ ;;; -0.16147897 -0.28478035 0.2872645; -0.049220085 -0.19612044 -0.01854634; 0.019390922 -0.18109517 0.09886145;;; -0.31781033 -0.25641158 -0.28214195; 0.0017100262 0.0045408225 0.22070143; -0.032420687 0.06659532 0.13606903;;; -0.2726938 0.085752875 -0.21413656; -0.21920969 -0.14661513 -0.1499309; 0.3573454 0.28001207 0.10929566;;;; -0.3413261 -0.14099704 0.1380114; -0.05061246 -0.2716685 -0.05580064; 0.17579815 -0.1003356 0.089005284;;; 0.08214059 0.08356265 0.045089237; 0.19048275 0.08633256 0.14742972; -0.008862632 -0.11897884 -0.01090846;;; -0.069584146 -0.22266361 -0.118842006; -0.19426495 -0.2343136 0.11473537; -0.39734897 -0.35600758 -0.16990645;;; â€¦ ;;; -0.03500889 0.052863535 -0.008689648; -0.062334932 -0.31454134 0.02023967; -0.1935436 -0.55353445 -0.051639948;;; 0.10363507 0.00082933216 0.01729982; 0.21427841 0.0055840886 0.17590776; -0.15806493 -0.3193291 -0.22030161;;; 0.39330554 0.20687243 0.016445708; 0.08818727 -0.016257755 -0.25651613; -0.15133502 -0.2180244 0.194826;;;; â€¦ ;;;; -0.19336788 -0.16148129 -0.13016704; -0.018108452 -0.3957148 -0.31751; -0.042325065 0.09977531 0.042090558;;; -0.33584112 -0.26488605 -0.05006079; -0.14910115 -0.11917831 -0.0711405; -0.11266108 -0.04380577 -0.043698765;;; -0.06220054 -0.17401363 -0.3116382; -0.10656181 -0.19955593 -0.17935233; -0.15338014 -0.28702104 -0.23823832;;; â€¦ ;;; -0.18650234 -0.04077268 -0.4521994; 0.33649665 0.22524415 0.21599677; -0.09090794 0.0430888 -0.18367764;;; -0.102126285 0.12094245 0.04707262; 0.1256304 0.10131987 0.13935381; 0.08572039 0.04473216 -0.0086151045;;; -0.21105842 0.11485491 -0.061486576; -0.10054752 0.083180234 0.6289774; 0.1228166 0.043564633 -0.1524427;;;; 0.08168843 -0.2525836 0.07898342; -0.29725605 0.14264555 -0.003377339; 0.1984341 -0.13327675 0.069092505;;; -0.101994544 -0.1113621 0.05369261; -0.044006277 -0.19044843 -0.2224783; -0.008572036 -0.0878737 -0.33977836;;; -0.060593925 -0.15267959 -0.04240643; -0.14725776 -0.21324198 -0.0051329755; -0.12991683 -0.20278986 -0.12521717;;; â€¦ ;;; -0.17989826 -0.12920101 -0.0040420005; 0.021639844 -0.61085427 0.13779616; -0.25021592 -0.09920291 0.071487054;;; -0.20533547 -0.12369199 -0.022465454; -0.16773704 -0.04931011 -0.16157953; 0.07910737 -0.06960961 -0.24706274;;; 0.017250452 -0.13776919 -0.3036211; 0.16250253 0.3170373 -0.009207426; -0.12054916 -0.09508146 -0.03775047;;;; -0.13251959 -0.3060353 -0.06087069; 0.10211833 0.27552345 0.078957245; -0.056013416 0.5892828 -0.010823413;;; 0.23204343 0.10966968 -0.024539122; 0.04906771 0.025740199 -0.07100269; 0.033203475 0.049631655 0.06599876;;; -0.021089565 0.09970705 0.15652792; 0.10530462 0.13897973 -0.057717085; -0.08409835 0.03566081 -0.17554252;;; â€¦ ;;; 0.07448062 -0.08670763 -0.103907876; 0.20162702 0.176546 0.036885936; 0.08742563 0.32949913 0.05786938;;; 0.18311715 0.14403574 -0.06397439; 0.113387905 0.12282298 0.096156046; 0.015832663 0.03095035 -0.032420777;;; 0.44953126 0.26113626 0.08970444; 0.21163477 -0.005072227 -0.07519763; 0.15363432 -0.07344475 0.044515178], bias = [0.0037876677;;; 0.0034065857;;; -0.00222296;;; â€¦ ;;; -0.0004217926;;; -0.002689639;;; 0.0010464474;;;;]), layer_3 = (scale = Float32[1.141692, 1.3037837, 1.0858198, 0.75982374, 1.2649181, 0.70173705, 0.49321407, 1.1644856, 1.3086592, 1.3109422  â€¦  1.0372673, 0.52054816, 0.65375155, 1.0906273, 1.1750824, 1.0451039, 0.7550774, 1.1976742, 1.0863465, 0.6860291], bias = Float32[0.05570003, -0.18261416, -0.17221433, -0.28526148, -0.028567249, -0.443366, -0.27704763, -0.40892032, 0.110051654, -0.10468095  â€¦  -0.21843272, -0.47189143, -0.35214403, 0.019844968, -0.24640383, -0.33910075, 0.025140027, -0.33566523, -0.23491712, -0.30799547]), layer_4 = (weight = [0.08127115 -0.13621485 -0.08791874; 0.12952514 0.23288676 -0.047415793; -0.024135966 0.24411657 -0.063011065;;; 0.14300054 0.07241967 0.14195247; -0.11889873 0.013910589 -0.029507946; 0.032291718 0.18197927 0.14305127;;; -0.17573263 -0.0035366777 0.27709654; -0.023183161 0.16557 0.08889151; 0.084712304 -0.26268864 0.42169493;;; â€¦ ;;; 0.45861608 0.06220275 -0.13876814; 0.09399442 -0.31527767 -0.2756219; -0.04620095 -0.10379705 0.033216115;;; -0.036388252 0.033217214 0.025205238; 0.09363288 0.019673016 -0.09997202; 0.09971803 -0.3715519 -0.031820726;;; 0.05795556 0.017186876 0.08122878; -0.05190012 -0.14108641 -0.22162041; -0.33053356 -0.023929428 0.37266755;;;; -0.23469053 -0.38453856 -0.33675888; -0.10559637 -0.43715596 -0.34274343; 0.10642616 0.118917175 -0.4221763;;; 0.30666 -0.37125117 -0.13247709; 0.055268474 -0.23690867 -0.09632069; 0.51625234 0.29702264 -0.051992647;;; 0.28313255 0.085623994 0.015369837; 0.7345945 0.096675955 -0.063912064; 0.1537034 0.0061621843 -0.123474136;;; â€¦ ;;; -0.308433 0.06327327 0.15244494; -0.14901537 -0.13104853 0.16011304; 0.18470334 -0.116398595 -0.1296396;;; -0.7078245 -0.05964385 -0.12523834; -0.24516712 -0.07317702 -0.170986; -0.03876266 -0.32697117 0.049751893;;; 0.107553065 -0.126644 -0.0994343; 0.28228495 -0.42596957 -0.06044795; 0.07125049 -0.3128294 0.14162794;;;; 0.06978148 0.0377907 0.086528935; 0.055086605 0.16130844 -0.1729545; -0.0050224345 0.45804808 0.16900583;;; -0.49679515 0.14770706 -0.35768914; 0.18796872 0.10290928 0.15244947; -0.18657722 0.33404982 0.19353385;;; -0.31549653 -0.08798633 -0.13173011; -0.3418088 -0.22824809 -0.29282945; 0.03885329 -0.41386855 -0.09702614;;; â€¦ ;;; -0.14851259 -0.10398052 -0.30585942; -0.09409638 0.23045172 0.15591535; 0.11548012 0.3121839 0.63635945;;; 0.11582114 -0.21269126 -0.031573243; 0.3113961 -0.03809347 -0.13795064; -0.07086924 -0.08301175 -0.27266255;;; 0.052967526 0.21792887 -0.278917; 0.35764956 0.0743654 -0.20594783; -0.035743006 -0.074832134 -0.30282867;;;; â€¦ ;;;; -0.004087527 -0.18397665 -0.15773557; 0.032589335 0.113287315 -0.01651739; -0.12425981 -0.44008344 0.10044385;;; 0.124416 -0.005690465 -0.27344668; 0.08875493 0.046029042 -0.17560884; -0.26163632 -0.118798 -0.1403634;;; -0.108385876 0.23739858 -0.26753247; -0.22493073 0.2488108 0.17895354; 0.10086312 0.007344768 -0.0574735;;; â€¦ ;;; 0.026716497 -0.21653455 0.3988844; -0.10333015 0.0061526326 -0.00817757; -0.18525071 -0.47031924 0.06356635;;; -0.14760013 -0.0061325342 -0.0022138292; -0.34242952 0.051894333 0.0014106979; -0.3253826 0.33071288 -0.4017308;;; 0.27003977 0.014481817 -0.080064; 0.08351705 -0.0020861505 -0.12444634; 0.119662054 0.050517976 -0.023998698;;;; 0.06668164 0.15189025 0.37647066; 0.2309424 -0.0688131 -0.2454423; -0.06350296 -0.51703185 -0.37636328;;; 0.10265307 0.2641563 0.15561444; 0.1849517 -0.093864664 -0.44926846; 0.23707816 0.13489191 0.40699703;;; 0.11491404 -0.0793634 -0.21545948; -0.11964103 0.46864069 0.42928803; -0.2776125 0.22106625 0.34547013;;; â€¦ ;;; 0.054769862 -0.37937623 0.1648618; 0.27040616 0.033493076 0.44160214; 0.042273883 -0.34650713 0.0011012251;;; 0.34909526 -0.33298144 -0.52585155; -0.009788461 0.25267708 -0.044692144; -0.3720889 0.03481024 -0.07921132;;; -0.09894704 -0.02950259 -0.15977815; -0.025155343 -0.12422006 -0.074853614; -0.052763358 -0.13954751 -0.1652741;;;; -0.04289263 0.025657179 -0.1135662; -0.014847833 -0.3825703 0.0733209; 0.06485526 0.16776192 -0.3142971;;; 0.06727499 0.23371579 0.42617574; -0.30131078 -0.4257646 -0.42361552; -0.06175878 0.051201187 0.18479817;;; -0.13246088 0.17134158 0.07045781; -0.13338391 0.2084 0.31441176; -0.0071202107 -0.13516887 0.3098655;;; â€¦ ;;; 0.11361763 0.13259755 0.16625494; 0.18169497 -0.068978235 -0.05179163; -0.038693044 -0.10201339 -0.19176623;;; -0.26970285 -0.18495414 -0.1709685; 0.09106934 -0.18576224 0.3265447; -0.1289107 -0.3821691 -0.051937215;;; -0.19398521 -0.015566728 0.14011152; 0.08978776 -0.05602325 0.038200878; 0.12368373 0.056419462 -0.21899898], bias = [0.001982946;;; -0.0017811939;;; -0.0010304294;;; â€¦ ;;; -0.0016537106;;; 0.0016145379;;; -0.0017626585;;;;]), layer_5 = (scale = Float32[0.8506596, 0.7960806, 0.87502265, 0.9435535, 0.5863778, 0.9484181, 0.8994239, 0.73934543, 0.6933726, 0.86053205  â€¦  0.80509907, 1.0262616, 0.9020979, 0.53722644, 0.82982326, 0.9105368, 1.059662, 1.0329177, 0.8010072, 1.1027323], bias = Float32[-0.15903485, -0.3620448, -0.38268495, -0.28565392, -0.50624174, -0.13310702, -0.24147701, -0.5744504, -0.29100618, -0.63378507  â€¦  -0.4974981, -0.09484989, -0.5001057, -0.40302986, -0.17844601, -0.24496187, -0.34335682, -0.1193172, -0.5368521, -0.28159535])), l4 = (layer_1 = NamedTuple(), layer_2 = (weight = [0.06666412 -0.017509896 0.3356048; -0.14840429 0.07855059 0.1890432; 0.09442456 0.10240754 -0.1201704;;; 0.06167082 0.07144958 -0.011721574; 0.3855126 0.16008665 0.08778511; -0.035927568 0.14434868 -0.029850123;;; 0.28911996 -0.12936462 -0.11163598; 0.1461357 0.09005024 -0.02782929; -0.064548604 -0.17609704 -0.11472965;;; â€¦ ;;; 0.108000875 -0.16748424 -0.1327048; 0.059102684 -0.070826136 -0.120248765; 0.26033345 0.027122976 -0.2412189;;; -0.23593344 -0.34057605 -0.28846744; -0.023905378 0.069270134 0.13408493; -0.063976504 0.07193024 0.032181896;;; 0.2933348 0.069810644 0.17476907; -0.1544674 0.100229174 -0.27465856; -0.06501169 -0.3666888 -0.310943;;;; 0.1504671 0.23292299 -0.16922991; -0.5150453 -0.11882772 -0.10341104; -0.110407054 0.04678976 0.31257874;;; -0.07938098 0.16078512 -0.14080721; 0.41591075 -0.014778908 0.2280495; 0.14206697 -0.23411998 -0.20250577;;; -0.15166312 -0.13929206 0.09637319; -0.29904667 -0.20600592 -0.06742041; -0.3026726 -0.37755114 -0.46569592;;; â€¦ ;;; -0.021242194 -0.043809704 -0.26910788; -0.19396926 0.34509876 0.2093708; 0.10091534 -0.17888479 -0.10712743;;; -0.032152675 -0.12911437 0.018703863; 0.012642628 0.35908973 0.07191688; -0.2302147 -0.5179638 -0.20417479;;; 0.24152169 -0.03678352 -0.22081971; -0.29446664 0.23491107 -0.1331742; -0.093784966 -0.39922166 0.14528836;;;; -0.19519423 -0.030114124 0.105251044; 0.32141292 -0.09197584 0.3696495; 0.2588532 0.31985465 0.22589783;;; -0.1957328 0.23842065 0.29122597; 0.033827607 0.02337424 -0.09761224; -0.22482069 0.006442592 0.1458891;;; -0.13721561 0.16353199 -0.23794983; 0.0035764005 -0.24156286 0.034035522; 0.014766836 -0.2580131 -0.17979877;;; â€¦ ;;; -0.19118708 -0.5243779 0.19000354; 0.028752066 -0.02241178 0.13212952; -0.47627443 -0.39334348 0.15494911;;; -0.17646186 -0.27306426 0.10582429; 0.025652483 0.13609163 -0.09078744; 0.1791516 0.19771457 0.40192762;;; 0.021222174 -0.061421946 -0.25264996; 0.11246922 0.076208174 -0.1067413; 0.11467305 0.16464247 0.090931736;;;; â€¦ ;;;; -0.1264097 -0.20112002 0.027899295; 0.20764121 -0.09690622 0.083505146; -0.120179385 0.20937629 0.047181554;;; 0.3427034 0.30170357 0.18764529; -0.090814866 -0.2130459 -0.13638343; -0.15066801 -0.2723977 -0.120723926;;; -0.080898665 -0.032653995 0.013445208; 0.10630802 -0.08786934 0.3073478; -0.02429249 0.19024493 0.07111441;;; â€¦ ;;; 0.21649958 -0.050385468 -0.08089929; 0.14739573 0.14752807 -0.48602515; 0.13861677 -0.07558398 -0.02389679;;; 0.14758274 0.10816078 0.26643488; -0.20847638 -0.14284983 -0.16937174; -0.13041252 -0.045515206 0.28167138;;; -0.25482458 -0.29996368 -0.05560935; 0.15628205 0.27812868 0.27000436; -0.044676233 -0.083180234 0.00063663913;;;; -0.35375676 0.028313968 -0.31028718; -0.088489845 -0.043167107 -0.11222461; -0.19925347 0.21104237 -0.116171174;;; 0.03453993 0.22610427 0.1262002; 0.14232564 -0.11249792 -0.16154978; 0.118169256 0.06353113 -0.33583978;;; 0.31854892 0.20671739 -0.0009905767; -0.037715785 -0.10129216 0.071764745; -0.037555188 -0.15091354 -0.03297721;;; â€¦ ;;; -0.038352165 0.26020452 0.017251296; 0.3742656 0.13479471 -0.13636401; 0.09350859 -0.03612981 0.036102578;;; -0.29528883 0.16104786 0.1258483; 0.25528663 -0.07905876 -0.14468016; 0.2116665 -0.17443532 -0.0995536;;; 0.017752789 0.27624437 -0.090674326; -0.09565222 0.062689565 0.15090176; -0.021031406 -0.01752774 0.0021972826;;;; -0.13668297 0.22213092 0.031103175; -0.018203788 -0.07075301 -0.25516012; 0.010921955 0.022991775 -0.32682723;;; -0.23843548 -0.023508528 -0.26941475; 0.22509377 0.04957089 -0.5876953; 0.08160889 0.043637127 -0.048434854;;; -0.034001812 -0.32565278 -0.050157208; -0.14461961 -0.04049425 0.13632019; -0.0650824 -0.13795735 0.17454481;;; â€¦ ;;; 0.16939488 -0.26369458 -0.32133812; -0.077650644 0.36636415 -0.09643001; -0.13231906 -0.31570435 0.23858458;;; -0.16177367 -0.2868849 -0.52104497; -0.09021853 0.17170475 -0.12085604; 0.15169407 0.13396662 0.34154543;;; -0.23542671 0.066342235 0.3759552; -0.24071863 -0.15109406 0.02635976; -0.16257277 -0.3809079 -0.37018844], bias = [-0.000552214;;; 9.736018f-5;;; 0.0003545269;;; â€¦ ;;; -0.00019954749;;; -0.000632167;;; 0.00032577946;;;;]), layer_3 = (scale = Float32[0.99390614, 0.84598255, 0.767882, 0.98168373, 0.7472981, 0.79529935, 0.8317161, 0.88966465, 0.6105536, 0.9222854  â€¦  1.0376741, 0.9031358, 0.81404287, 0.7625499, 0.7690077, 0.86665297, 0.8838044, 0.8909728, 0.90542936, 0.95299774], bias = Float32[-0.23494384, -0.5131665, -0.30298337, -0.48607203, -0.6647923, -0.5582143, -0.54298025, -0.4283251, -0.5663835, -0.46991387  â€¦  -0.24337375, -0.46603355, -0.16102721, -0.39671284, -0.60903805, -0.28075513, -0.2274809, -0.43280876, -0.3731548, -0.1583038]), layer_4 = (weight = [0.13408197 0.019323085 -0.070639506; -0.25884947 -0.15684886 0.019249426; 0.08111664 0.061924554 0.033449806;;; -0.06475236 -0.15336333 -0.08828218; 0.085781805 -0.17880508 0.35330904; 0.06923469 0.23801593 0.19340529;;; -0.2440952 -0.17648342 0.1170757; 0.21323779 -0.34505868 0.32186916; 0.04186193 -0.003375683 0.12538251;;; â€¦ ;;; -0.15185778 -0.2989338 0.070710555; 0.20144555 0.089105256 0.07553313; -0.07622818 0.2474577 0.21047671;;; -0.026362335 0.032717373 0.101786144; 0.12594873 -0.17194262 0.012562941; -0.013448778 -0.0047708796 0.20435718;;; 0.15095131 0.32382014 -0.29926842; 0.2068005 0.102055736 -0.24999955; -0.048612896 0.15729421 0.08601618;;;; 0.08529572 0.0085764015 0.049136594; -0.17861317 0.18981493 0.39737174; 0.14227575 0.056513943 -0.41657615;;; 0.058698278 0.121232815 0.032822672; 0.12833722 0.07337261 0.2301079; 0.2508834 0.25915647 0.046745867;;; 0.09548726 0.08029185 -0.3041827; -0.12767504 -0.20283711 -0.36161777; 0.05988193 -0.067817934 -0.17777212;;; â€¦ ;;; 0.11299395 -0.5464335 -0.5633215; -0.16430603 -0.648379 -0.24095486; 0.22536382 0.068676844 -0.056206536;;; -0.058631387 0.041399628 -0.05871833; -0.19068334 0.082874484 -0.053025845; -0.23768203 -0.109579146 -0.22819833;;; -0.29935217 -0.053354394 0.119979374; -0.0035130023 0.18104032 -0.16816892; -0.14407669 0.033119816 -0.07424786;;;; -0.19646634 0.19424632 0.047078982; -0.039222594 0.027499327 0.27744946; -0.1601913 -0.18336754 0.25002322;;; -0.22163506 0.043340515 0.18606882; -0.22163302 -0.2860321 -0.26292053; -0.3405666 0.025689248 -0.05459502;;; -0.30323017 -0.14041406 -0.134296; -0.03887988 -0.13782494 -0.05357751; -0.15812643 -0.28184792 -0.6150978;;; â€¦ ;;; 0.3083446 -0.032840226 0.08145403; 0.36996698 -0.03126019 -0.17723979; 0.055164013 -0.21707436 0.017760575;;; 0.21118316 -0.017902626 0.12972295; 0.18323755 0.06388105 -0.28449023; -0.3713954 -0.35702002 0.2143665;;; -0.017721336 0.1664381 0.0948055; -0.02170355 0.07767015 0.07166338; -0.055071343 0.0013176703 0.08810492;;;; â€¦ ;;;; 0.22986346 0.008352187 -0.12616552; -0.3982723 0.06562734 0.15118821; 0.1647645 0.0577881 -0.21097532;;; -0.19005801 0.64440995 -0.25383136; -0.093940794 0.17798069 -0.22277538; -0.018588029 -0.2364412 0.11390903;;; -0.32878694 0.23826216 0.27322835; 0.1710448 -0.019274222 0.19442734; 0.38657078 0.03125442 0.23194541;;; â€¦ ;;; -0.6668885 0.15234801 0.28877538; -0.3508261 0.202049 0.15481848; -0.046025548 0.027712682 -0.35635376;;; -0.07919877 0.33607015 -0.2088253; 0.21525167 0.3868775 -0.095742166; 0.18334621 -0.13093638 0.045450322;;; 0.05127635 -0.013286928 -0.07047892; -0.16169424 -0.067931324 -0.2494995; 0.2425517 -0.09927348 -0.36236212;;;; -0.12100184 -0.12384538 -0.11404389; 0.4083822 0.11715248 -0.24792911; -0.24979584 -0.13865265 0.0048164073;;; 0.2818818 0.22715667 0.4134558; -0.023420025 0.15326117 0.109083064; -0.371336 -0.11424162 -0.058020893;;; -0.30539426 -0.18925093 0.099922515; -0.060179118 -0.22758156 -0.31448823; -0.45634633 -0.2143124 -0.3416762;;; â€¦ ;;; 0.23709957 0.12119886 -0.033284016; -0.002964084 -0.047751106 -0.2737694; -0.4149984 -0.18695769 0.092400394;;; 0.11221692 -0.06889977 -0.07386861; -0.5115274 -0.4197026 0.084301025; -0.29923657 -0.011675132 0.23196217;;; -0.20827578 -0.022344401 0.10719316; -0.05233365 -0.04366367 0.29905728; 0.4162102 0.010232646 -0.51928765;;;; 0.20115572 0.0062683215 -0.11658396; 0.15160972 0.07986662 0.16364379; 0.0015285715 0.017330445 0.0843015;;; 0.035953518 0.008691969 0.01927702; 0.05858064 0.12199798 0.032409664; 0.23118046 0.1315001 0.2906628;;; -0.056764014 -0.3138764 0.029045906; -0.31655815 -0.19706516 -0.04988666; -0.1448903 -0.4468677 -0.2957963;;; â€¦ ;;; -0.03240861 0.20513977 0.28236708; 0.051649 -0.13386983 0.043127153; -0.03803286 0.20108232 -0.116373844;;; -0.16002929 0.05598115 -0.013700911; 0.12786314 0.14144689 -0.056729574; 0.27511433 0.050268926 0.14942746;;; 0.11813863 0.059569493 0.3401835; -0.047880698 0.34865183 0.09089664; 0.14709166 0.01959204 0.2661548], bias = [4.721794f-5;;; -0.00013067243;;; -0.00016221011;;; â€¦ ;;; 0.0003533059;;; 1.4093869f-5;;; -6.5466316f-5;;;;]), layer_5 = (scale = Float32[0.89831424, 0.98153406, 0.9366373, 0.93842226, 1.1039639, 0.97565037, 0.9332473, 0.9040304, 0.9453757, 0.86765087  â€¦  0.89641464, 1.0304639, 0.86877483, 0.7795987, 0.8497489, 0.9146983, 0.9057621, 1.094501, 0.96101767, 0.8923554], bias = Float32[-0.5921174, -0.46193567, -0.5433685, -0.27124637, -0.40085363, -0.5647397, -0.4397073, -0.4767037, -0.5096473, -0.39289498  â€¦  -0.44217068, -0.55684525, -0.3747128, -0.39359963, -0.37303114, -0.39062047, -0.516743, -0.5365629, -0.4257232, -0.29947352]), layer_6 = (weight = [0.29971874 -0.09373633; 0.22710796 0.08034449;;; 0.16906247 0.6769817; 0.12249467 -0.07314482;;; -0.00020777281 0.08212089; -0.041644644 0.0994156;;; â€¦ ;;; -0.30102357 0.198694; -0.058667373 -0.1141933;;; 0.22893807 0.29744932; 0.10248346 -0.3932469;;; -0.09146523 0.09095131; 0.16988122 0.3531903;;;; -0.26974925 -0.33801737; -0.099595845 -0.06050841;;; 0.37895092 0.007798907; -0.20719944 0.2664283;;; 0.080128066 -0.21086207; -0.028650265 -0.19659488;;; â€¦ ;;; -0.24383883 0.19348302; -0.1298657 -0.13325278;;; -0.15021206 -0.08139781; -0.019235017 0.022075938;;; -0.09057981 0.13703933; -0.14675842 -0.17729504;;;; 0.15732032 0.24589118; -0.01851205 0.23545556;;; -0.048028454 0.080639295; 0.027909672 -0.0859927;;; 0.21939693 0.255363; -0.020026874 -0.0018053222;;; â€¦ ;;; -0.5195609 -0.64262354; 0.029390654 0.22380418;;; 0.08956845 0.22774564; 0.2648991 0.082345136;;; 0.53636783 -0.4492535; 0.04442659 0.12105904;;;; â€¦ ;;;; -0.037008036 0.13284309; 0.040940847 -0.3329092;;; 0.4003234 -0.2094836; 0.21893749 0.118886;;; -0.10594709 -0.12783216; 0.27207956 0.009608278;;; â€¦ ;;; -0.15905574 0.43552005; -0.047938988 0.06140723;;; -0.67827195 -0.47971246; -0.6548882 -0.5431981;;; -0.49979895 0.066812694; -0.68921393 -0.3310191;;;; -0.10806065 0.14192311; -0.21532264 -0.13283373;;; 0.11582157 0.00982412; -0.40545115 -0.24318634;;; 0.1327898 0.025735121; 0.031788085 0.105206326;;; â€¦ ;;; -0.19459757 0.22132826; 0.1951138 0.4214213;;; 0.40734267 -0.17955044; -0.071321584 -0.1459295;;; 0.07184637 -0.056745633; -0.0074473415 0.005813206;;;; 0.17238697 -0.08663996; 0.121794 0.122546546;;; 0.37201953 -0.08124149; -0.21121223 0.12527584;;; -0.15441982 0.27478522; 0.2188998 0.020827478;;; â€¦ ;;; -0.4195822 0.19992463; -0.18333578 0.07352052;;; -0.20114738 0.020137046; -0.2508837 -0.17859717;;; -0.060583513 0.10749578; 0.17204888 -0.10310797], bias = [-0.00015838176;;; 0.0002255537;;; 0.0012144623;;; â€¦ ;;; -0.00045146566;;; 0.0013652684;;; -0.0036247594;;;;]), layer_7 = (scale = Float32[1.2405202, 0.9365424, 1.0005431, 1.0575212, 1.3723897, 1.3711503, 1.582252, 1.4030762, 1.0273268, 1.6347506  â€¦  1.3417401, 1.3335898, 1.0081694, 1.4216919, 1.2628305, 0.9996996, 1.1750305, 1.409872, 1.4471956, 1.217536], bias = Float32[-0.2668244, -0.15388875, -0.50066185, -0.25189403, -0.3374007, -0.34481233, -0.2949749, -0.32014504, -0.31067267, -0.5610759  â€¦  -0.45956403, -0.23259029, -0.5587866, -0.3470795, -0.42791864, -0.52467257, -0.28527158, -0.47625625, -0.42458615, -0.43026876])), l5 = (layer_1 = (weight = [0.6472401 0.16103242 0.11167205; 0.032197066 0.029116822 0.27087435; 0.061876882 0.3348036 0.286279;;; 0.14949897 0.09075191 -0.33505225; 0.03907691 -0.25720116 0.023076827; -0.2806386 0.0019397864 -0.26489028;;; 0.38620806 0.026047718 -0.24007444; -0.15605626 -0.08254219 0.2614682; 0.07137513 -0.062260747 0.12853265;;; â€¦ ;;; -0.08138931 -0.2969091 0.19443011; -0.066360086 -0.3042499 -0.11642932; 0.16570786 -0.23962566 -0.1628939;;; 0.01135905 0.37932763 -0.016884893; 0.21876256 0.16986983 0.23859374; 0.28027767 0.05339162 0.17487636;;; -0.559192 0.15902087 -0.07521671; -0.22334197 -0.107259154 0.09676389; -0.38263795 -0.16991767 -0.47012722;;;; 0.30273405 0.02058737 0.013464101; -0.13504547 -0.07647241 -0.02809687; -0.36640564 -0.07265039 0.2089916;;; -0.104453966 -0.1341667 -0.5044284; -0.4676545 -0.21806283 -0.41320124; -0.14497133 -0.26656437 -0.5710984;;; 0.34009296 0.27192417 0.14166978; -0.116212904 0.10129125 0.13766803; 0.011657913 -0.16145907 0.26172385;;; â€¦ ;;; -0.075667396 0.110805675 -0.1404414; -0.29392397 -0.0031218496 -0.097472474; -0.4885068 -0.24303044 -0.35591263;;; -0.045284964 0.1043069 0.25402272; 0.114971586 -0.047286782 -0.0703576; -0.14997832 -0.15552762 -0.19397402;;; -0.26394594 0.17936727 0.1253721; -0.27325562 -0.07164455 -0.08736883; -0.18904547 -0.04120144 -0.22317947;;;; -0.037109364 -0.373782 -0.11371711; 0.13237327 -0.14311117 0.04852928; -0.052798007 0.10650123 -0.03654357;;; 0.09750432 -0.13803464 -0.13322218; -0.2305639 -0.11498211 -0.10819807; 0.058603138 0.05150015 0.110734455;;; 0.25934723 0.0076804236 0.08660261; -0.27804866 -0.1477193 -0.13928333; -0.0887311 -0.09081771 0.13667105;;; â€¦ ;;; -0.07033125 -0.57772386 0.07725706; 0.31117973 -0.0067935926 -0.5134311; 0.24393693 -0.33518776 -0.40047678;;; 0.018160982 0.0011816658 -0.28077936; 0.055277396 -0.01951736 -0.036860354; 0.3882478 -0.021764195 0.0124572655;;; 0.29093787 0.17068247 -0.21287295; 0.24606813 -0.0057463916 0.23257618; 0.32259744 0.10760552 0.23857273;;;; â€¦ ;;;; -0.19387051 -0.35498616 -0.12631822; -0.62471545 -0.6327882 -0.3899862; -0.5594236 -0.58406556 -0.4191239;;; -0.24887592 0.08349327 -0.54564536; -0.22665398 -0.28913578 -0.2896207; -0.3532439 -0.14835013 -0.5873094;;; 0.053111885 -0.014421168 0.059904214; -0.06733059 0.03224885 0.18707125; -0.059448432 -0.033555135 0.044321623;;; â€¦ ;;; -0.018771531 -0.03219635 -0.13733245; -0.005307363 0.00018841159 -0.19639747; 0.30149966 0.3191587 -0.25292867;;; -0.18783632 -0.095559314 0.06802619; 0.04378163 0.045599103 0.14762798; -0.12747239 0.20734301 0.36205298;;; -0.044748828 -0.0005124052 -0.14489138; -0.27262092 -0.17897387 -0.30450973; 0.0037950266 -0.30825862 -0.3730241;;;; -0.09232299 0.07068223 0.2599951; -0.49897397 -0.11827669 0.043486677; -0.17186761 -0.010425473 0.075615235;;; -0.20017934 -0.061376434 -0.15752468; -0.118669644 -0.21366732 -0.121164404; -0.03957907 0.21661252 -0.091063656;;; 0.100896135 -0.062995724 0.16111071; -0.028119592 -0.11828323 0.057752084; -0.07373627 -0.079057604 -0.013080183;;; â€¦ ;;; -0.20619158 -0.017845921 0.023102535; 0.06789779 0.12717202 0.01534499; -0.05349345 -0.13182718 0.20650983;;; 0.09506294 -0.11649836 -0.04540322; 0.1873673 0.031328324 -0.07944476; 0.16642681 0.052833702 -0.0096975025;;; 0.32762954 0.32063627 -0.10864287; 0.00609955 0.09060764 -0.008952301; 0.25298387 0.20705871 -0.0012191972;;;; -0.295836 -0.070302494 -0.026737673; 0.019651912 0.35913336 -0.33706704; 0.20416021 -0.11156295 -0.21161136;;; 0.08517477 0.025444109 0.08567232; 0.034712203 0.08990728 0.105497025; -0.28645092 0.17348887 -0.20173763;;; -0.5375273 -0.27154237 -0.20515603; -0.12872511 -0.25757003 -0.13259262; -0.04368654 -0.1830744 -0.11118658;;; â€¦ ;;; -0.32296178 -0.07148443 -0.25785276; 0.007140261 -0.028211849 -0.01919566; 0.20198935 0.115080275 0.21595407;;; 0.15517956 -0.028584942 -0.20463477; -0.12183195 -0.32629824 -0.19385104; 0.24347146 -0.13970748 -0.21268037;;; 0.019619988 -0.07937632 -0.032487452; 0.22329482 0.18249959 -0.18002889; 0.2165761 0.06860286 -0.06973463], bias = [-0.00012909698;;; 1.8881741f-5;;; 0.00012136789;;; â€¦ ;;; 0.00016477951;;; -1.328696f-5;;; 0.00012859325;;;;]), layer_2 = (scale = Float32[0.87921864, 0.6934717, 0.9695786, 0.95100015, 1.2614365, 1.055301, 0.5706202, 0.79041106, 1.0879465, 1.4238529  â€¦  0.9180551, 0.8369875, 0.9812865, 0.6824153, 1.006172, 0.85855275, 0.6640294, 0.63563514, 0.9526655, 1.0161263], bias = Float32[-0.45953074, -0.21388815, -0.41878906, -0.74581754, -0.83252484, -0.23917636, -0.28993323, -0.32419795, -0.71936995, -1.0548921  â€¦  -0.420999, -0.11880126, -0.86678106, -0.23436984, -0.22145669, -0.4089261, -0.1990587, -0.16274318, -0.51395833, -0.3456255]), layer_3 = (weight = [-0.27535638 0.08599331 -0.15255675; -0.14921145 0.017599715 -0.07815982; -0.23654616 0.08393422 -0.22167648;;; -0.23628871 -0.023138264 -0.253654; -0.19922182 -0.14008357 -0.26675764; -0.22933392 -0.15342893 -0.2891283;;; -0.11830001 -0.07053912 -0.083680555; -0.087122835 -0.08826325 0.06632892; -0.089325756 -0.0994178 0.13486673;;; â€¦ ;;; -0.3507617 -0.23827656 -0.41758463; -0.34947994 -0.3998968 -0.38323134; -0.21147405 -0.29553035 -0.26489487;;; -0.60304886 -0.16734996 -0.12610409; -0.40145135 -0.1520165 -0.3214263; -0.81353915 -0.34288207 -0.6207156;;; 0.026815202 -0.3165182 -0.14570946; -0.003047917 -0.105396196 -0.2043775; -0.09885428 -0.2544458 -0.115620516;;;; 0.19776347 0.33426663 0.14148371; 0.55372256 0.18314363 0.4196633; 0.19498156 0.3395788 0.18311396;;; -0.30380103 -0.25225013 -0.13281457; -0.23219022 -0.22244358 -0.26388106; -0.2536173 -0.30480507 -0.32833007;;; 0.22035028 0.29064178 0.1709615; 0.21753067 0.26332223 0.2849395; 0.09530779 0.12501757 0.25052205;;; â€¦ ;;; -0.01925732 0.09139746 -0.11816523; 0.07170554 -0.045311484 -0.068182126; -0.11935206 -0.17402646 -0.09766043;;; 0.08101304 0.24716748 0.051481087; 0.30107403 0.20603672 0.24592146; 0.08359857 0.114996284 0.16535099;;; 0.13055551 0.15213586 0.2874469; 0.015632872 0.22484463 0.08558808; 0.24517523 0.19497815 0.15947483;;;; -0.27941844 -0.09300204 -0.2672665; -0.13309544 0.14162229 -0.072601944; 0.0825311 0.3193397 -0.0039402684;;; 0.11221973 -0.030935843 0.020183342; 0.12441001 0.16653512 0.2597383; 0.022435334 -0.006526146 -0.20800179;;; -0.047923394 -0.20858602 0.21595737; 0.23397723 0.13177161 0.14272973; 0.12649699 0.0674545 0.057835467;;; â€¦ ;;; -0.10104032 -0.059290893 -0.21266764; -0.10677404 -0.1672389 -0.016743492; -0.17455946 -0.28819272 -0.26731083;;; 0.046107728 0.018258194 0.0359878; -0.12102516 -0.12968276 -0.04999106; 0.0836427 0.06907166 0.21330322;;; 0.1466187 -0.4212539 -0.2668458; 0.20711057 0.12968175 0.08920024; 0.21806847 0.19324785 0.24129212;;;; â€¦ ;;;; 0.16510323 0.123629235 0.0021029452; -0.20505124 -0.22916768 -0.33529833; -0.5393531 -0.25222495 -0.27212915;;; -0.22627561 0.11105413 0.018924953; -0.23599698 0.094424255 0.127894; -0.46023455 -0.20603281 -0.050215404;;; 0.35351542 -0.009296882 -0.27509817; 0.17649885 0.18876089 0.016817732; 0.1563259 0.07601879 0.030498616;;; â€¦ ;;; 0.09393678 0.20552182 0.032436125; -0.0034673798 -0.007849164 -0.08829108; -0.13226463 -0.10501962 -0.04657839;;; 0.7347622 0.5202905 0.40768966; 0.027756875 0.15551983 -0.0054834154; -0.2177923 -0.08452607 -0.12040093;;; 0.293355 -0.26455617 -0.4893661; 0.27546066 -0.25988135 -0.23932904; 0.25131738 0.011103321 -0.087372564;;;; -0.4549623 -0.20522217 -0.28918862; -0.35462114 0.033793308 -0.30205354; -0.43912587 -0.27511522 -0.5509707;;; -0.17274022 0.05286316 -0.024934547; -0.005938747 -0.028573751 -0.20032921; -0.0044929716 -0.03264917 -0.2054307;;; 0.058378324 -0.20462124 -0.0033547992; -0.02655596 -0.26057413 -0.16437437; -0.033811025 -0.05459301 -0.32109576;;; â€¦ ;;; -0.15900254 -0.04313682 0.0931824; -0.1838385 -0.049526986 0.0034280282; -0.19608304 -0.014981722 -0.00016208338;;; -0.0161355 0.15110676 0.24056765; -0.02133464 0.096751206 -0.104484834; -0.18986884 0.06327421 -0.08747675;;; 0.15017067 -0.2603515 -0.25239015; -0.10372367 -0.34148312 -0.32049704; 0.015861088 -0.242927 -0.24424635;;;; -0.04194289 -0.000481569 -0.02722676; -0.0902401 -0.010691847 -0.0053968364; 0.026002573 -0.16597606 -0.08544607;;; -0.14359424 -0.22452945 -0.10913671; -0.13393764 -0.036599148 0.013358212; -0.008593529 0.085401766 0.038512938;;; -0.3447655 -0.14392371 -0.20279844; -0.3852002 -0.22072527 0.004426753; -0.3862676 -0.1329316 -0.0065502855;;; â€¦ ;;; -0.09926183 -0.11692279 0.022583613; 0.0024267908 -0.08206513 0.059030227; -0.07769654 -0.09475404 0.016395004;;; 0.0044087335 -0.2887285 -0.5224599; -0.11931646 0.037006773 0.0020299323; -0.114090316 0.012270138 0.033010304;;; 0.014256958 -0.13129875 -0.19558576; -0.1754355 -0.08467978 -0.2585786; -0.2716415 -0.27082133 -0.25692433], bias = [-0.00019907647;;; -0.000116968935;;; 7.4270756f-5;;; â€¦ ;;; -0.00018616361;;; -0.00016596026;;; 5.8354606f-5;;;;]), layer_4 = (scale = Float32[0.95142335, 1.0207562, 0.9377058, 0.91333616, 0.8188917, 1.2297442, 1.0199189, 1.0667442, 1.2092913, 0.9171896  â€¦  1.149001, 0.9060277, 1.0903488, 0.8029791, 0.85574585, 0.9007865, 0.92493653, 1.1018417, 0.9591627, 1.0281963], bias = Float32[-0.14084604, -0.5066658, -0.22265193, -0.5659598, -0.42220765, 0.040080752, -0.31317842, -0.18744077, 0.020041123, -0.34274  â€¦  -0.0146260895, -0.521988, -0.2524493, -0.4197942, -0.4509933, -0.62291986, -0.08453477, -0.5716648, 0.0044308812, 0.2616891]), layer_5 = (weight = [-0.15316597 -0.26703584; 0.051147416 -0.17540328;;; -0.027584711 -0.26058453; -0.35954508 -0.1984989;;; -0.3968722 -0.31683856; 0.10507847 0.34405336;;; â€¦ ;;; -0.107030615 0.011637756; 0.061035816 0.043479726;;; 0.14790508 0.15014625; 0.27777773 0.0050460487;;; -0.024381597 -0.07617341; -0.0037381873 -0.11805728;;;; -0.081783146 -0.076222114; -0.13148698 -0.058131844;;; -0.054326862 0.065991886; 0.05826363 0.012669984;;; -0.38102576 -0.01608269; -0.12723647 0.30030787;;; â€¦ ;;; -0.11951535 -0.055991385; 0.013886888 -0.1790372;;; -0.08164776 -0.16842191; -0.15652695 -0.3206807;;; -0.11681971 0.13622877; -0.279552 -0.10160103;;;; -0.30972436 -0.009490275; -0.15756603 -0.20521015;;; 0.1910555 -0.26893324; 0.119227156 -0.22267264;;; 0.3001893 -0.107216716; -0.0073732706 0.34951517;;; â€¦ ;;; -0.32509965 -0.26598713; -0.12357374 -0.07815415;;; 0.006109217 -0.18509027; 0.15255912 -0.081625625;;; -0.075240694 -0.029549263; -0.09548234 0.123352274;;;; â€¦ ;;;; -0.41138056 -0.38883844; -0.4647607 -0.33899277;;; 0.13462918 -0.06884962; 0.015048028 0.29329854;;; 0.019303996 0.026114276; -0.17243497 0.15268351;;; â€¦ ;;; -0.1842335 -0.15962194; -0.20243335 -0.06346771;;; 0.18389738 -0.12931938; 0.037523407 -0.032378346;;; 0.12603107 -0.18002142; -0.13800816 0.020138524;;;; 0.2020634 -0.1397833; 0.025307674 -0.01707663;;; 0.16363998 -0.28824055; 0.028497437 0.08086593;;; -0.35384682 -0.4416132; 0.12378126 0.21219473;;; â€¦ ;;; 0.0985423 -0.30415213; 0.07870632 0.28377652;;; -0.10539977 -0.42845973; 0.24291708 -0.2574928;;; -0.28223243 -0.12934734; -0.22790085 -0.18925895;;;; 0.16296953 0.2300379; 0.0539714 -0.022960572;;; 0.099085696 -0.030098483; 0.107798375 -0.08787887;;; 0.20785715 0.033108935; -0.076090366 0.081480235;;; â€¦ ;;; 0.3545618 0.016527269; 0.09816889 0.23056667;;; -0.0700978 -0.23087023; -0.08511161 -0.26542187;;; 0.10112853 0.011933311; -0.1430507 0.38923568], bias = [0.0007748349;;; -0.0013706337;;; 0.0012258062;;; â€¦ ;;; -0.0017700157;;; -4.6568864f-5;;; -0.0029137074;;;;]), layer_6 = (scale = Float32[1.170238, 0.86283547, 0.9132601, 1.0714835, 1.254481, 1.0143375, 0.8460798, 1.0280627, 1.1631649, 0.80179816  â€¦  0.9997957, 1.0338305, 1.1735971, 1.1730283, 1.1077955, 1.2496185, 1.0130601, 1.0621126, 1.2547958, 1.2032841], bias = Float32[0.2924032, -0.38188806, -0.40328524, -0.0015510273, -0.38562447, -0.61048275, -0.42424163, -0.9111069, -0.6313595, -0.3852724  â€¦  -0.3560073, -0.52650404, 0.042503778, -0.43023625, -0.8730383, -0.71818936, -0.46191952, 0.06342326, -0.6266645, -0.10959632])), l6 = (layer_1 = (weight = [-0.08751759 0.15228291 0.07159515; -0.04180162 0.17485352 0.14445344; 0.114419684 0.0727894 0.16974738;;; -0.054013267 0.077270634 -0.022597691; -0.09651464 -0.03814459 -0.092659205; -0.030676112 -0.01912313 -0.19004335;;; -0.41111624 -0.024232943 0.06403526; -0.65169036 -0.0035740477 0.40610576; -1.1215879 0.22354145 0.15774305;;; â€¦ ;;; -0.01893643 0.35697994 0.045986827; -0.723384 -0.32773608 0.13491084; -0.10603472 0.029157229 0.14030743;;; -0.19013906 0.13408914 -0.016552674; 0.00024189758 0.06586823 0.05775226; -0.51288575 -0.46924156 -0.2653138;;; 0.5157849 0.20166409 -0.26149094; -0.068594515 -0.07741287 0.36874372; -0.38132513 -0.07772086 0.068531446;;;; 0.13421638 0.0913955 0.1827396; -0.029009873 -0.12839065 0.10039498; 0.10842748 0.03925395 0.25345254;;; -0.5005857 -0.22149841 -0.15522386; -0.46435982 0.031303525 -0.1549124; -0.26243514 0.1618772 -0.2002148;;; -0.31211212 -0.04604626 -0.23573239; -0.24767368 -0.07416168 -0.23318814; -0.2565957 -0.12994419 -0.05079823;;; â€¦ ;;; 0.054407567 -0.042456742 -0.010415405; 0.11779769 0.043918584 0.023675414; 0.30273768 0.06288563 0.14425403;;; -0.018585863 -0.0761158 -0.09775623; -0.0031519702 -0.026897723 -0.0018366195; 0.13081679 0.09593376 -0.04449357;;; 0.28899053 0.2467058 0.23040313; 0.08147478 0.14327176 0.050281174; 0.2757833 0.09373765 0.057031542;;;; -0.05980429 0.1485888 0.101282604; 0.015826508 0.11218981 0.056670446; -0.025432127 0.09578595 0.044105295;;; 0.17067139 -0.20862915 -0.19295914; 0.24099572 -0.06708146 -0.026641214; 0.24859963 -0.11650569 -0.18452759;;; -0.1655941 -0.15857938 -0.0844522; -0.12899095 -0.36745766 -0.025979573; 0.030437287 -0.20956895 -0.019014014;;; â€¦ ;;; -0.030847592 0.032343373 0.29994994; -0.094442554 -0.34919238 -0.07159526; -0.0041578664 -0.026687212 -0.05150783;;; -0.036923874 -0.23318346 0.09219653; -0.15952145 -0.08817945 -0.038860336; -0.17551126 -0.13339734 -0.15762696;;; 0.10378798 0.03689725 0.004143894; -0.3512591 -0.32368892 -0.30309734; -0.37305957 -0.54004455 -0.54908895;;;; â€¦ ;;;; -0.104209214 0.047444697 0.18127514; -0.15698697 0.006908076 0.033055447; 0.17369059 -0.07182041 0.11899036;;; -0.09920756 0.04660156 -0.032361183; 0.054304678 0.068385035 -0.04749605; -0.08495315 -0.05490624 -0.22501579;;; -0.3614803 -0.029806435 -0.13628606; -0.54976547 -0.023736749 -0.34663677; -0.50329876 0.028130095 -0.3197957;;; â€¦ ;;; 0.04342611 -0.4077096 -0.059485566; -0.41677642 -0.6360338 -0.4907992; 0.060003117 0.28954872 0.10787802;;; -0.31803852 -0.364773 -0.044056993; 0.059332307 -0.029252546 0.12490927; 0.22828165 0.04029895 0.3647694;;; 0.356609 0.3807805 0.17391591; -0.41670632 -0.4543408 -0.20356098; -0.21574612 -0.2524905 -0.03880991;;;; -0.10399553 -0.22201201 -0.10372827; -0.115336224 -0.16937886 -0.13993588; -0.18874706 -0.1824198 -0.112555034;;; 0.00882613 0.087096035 0.103509456; -0.055462442 0.10025044 0.093694195; -0.20077544 -0.0061343266 0.23066217;;; -0.7162995 -0.20822354 -0.58458036; -0.49621907 -0.15669043 -0.31107497; -0.7626554 -0.2869756 -0.5533386;;; â€¦ ;;; 0.009703016 -0.040793497 0.06618704; 0.03476094 -0.118045576 -0.08838983; -0.086062975 -0.099385105 -0.03307495;;; -0.09729451 -0.109994784 -0.07704884; -0.14689837 -0.08520158 -0.102264754; -0.12812085 -0.20763004 -0.17496873;;; 0.09000077 0.08114848 0.1974115; 0.14045756 0.12827837 0.020716209; -0.1086603 -0.12198716 -0.21924119;;;; -0.12261462 -0.034056406 0.058167893; -0.13432543 -0.16652988 -0.04587445; -0.08771758 -0.14023247 0.0047801454;;; -0.5256192 -0.41863257 -0.25804305; -0.40415266 -0.30229527 -0.24137266; -0.35572118 -0.37099525 -0.23120472;;; -0.5242871 -0.42054555 -0.5030984; -0.41302103 -0.32722634 -0.42738783; -0.35810342 -0.23158239 -0.41001275;;; â€¦ ;;; -0.13426495 -0.05508511 -0.05993426; -0.1184335 0.004447921 -0.13547555; 0.055702854 0.042158537 -0.036259558;;; -0.010312206 -0.11207848 -0.036240138; -0.010488663 0.00028855525 0.051700734; 0.004449445 -0.025740044 -0.037104357;;; 0.042080387 0.0045792065 -0.014403514; -0.0061803847 0.0841764 0.016533569; 0.055210926 -0.013471297 0.09592577], bias = [-0.0005066542;;; -0.00013161852;;; 0.0003729574;;; â€¦ ;;; 0.0005410725;;; -0.00087025657;;; -0.0020609193;;;;]), layer_2 = (scale = Float32[1.0563179, 0.7480765, 0.7076593, 0.9667133, 1.0140259, 0.80458206, 0.7711264, 1.1252487, 1.1260954, 0.91491956  â€¦  0.9239727, 1.0638553, 0.9671171, 0.95800227, 0.8856542, 1.2385924, 0.94142526, 0.9560837, 0.98805887, 1.0294306], bias = Float32[-0.8096308, -0.29541153, -0.2248785, -0.07713597, -0.34503236, -1.0267967, -1.1085737, -0.27086747, -0.30196518, -0.6190323  â€¦  0.1397326, 0.07759185, 0.08312957, -0.58752805, -0.25844875, 0.097333364, -1.242126, -0.45200977, -0.019880142, -0.073205255]), layer_3 = (weight = [-0.45419994 0.038914077 0.16815983; -0.37942198 0.2734908 -0.0118994; -0.18514155 -0.037877746 0.0745251;;; 0.12320294 0.18564321 0.09043259; 0.16546987 0.057540752 0.098440334; 0.13204445 0.025669048 0.06183494;;; 0.059756417 -0.0793126 -0.111670785; 0.21272206 0.10026813 -0.039555177; 0.20162722 0.10731974 0.1193558;;; â€¦ ;;; -0.15216407 -0.089282975 -0.060906436; -0.023659887 0.06528431 0.11916135; 0.014295564 0.05230001 0.20118047;;; -0.51387674 -0.41075334 -0.40244636; -0.53796875 -0.25386634 -0.29470435; -0.65151364 -0.40820602 -0.44630173;;; 0.17232962 0.10528466 0.25584844; 0.09897901 0.106506 0.059033073; -0.0018941023 -0.10093619 0.14626162;;;; 0.20219065 0.19877024 0.020095464; 0.12557925 -0.010903966 0.20669313; -0.21403728 -0.16206981 0.36144155;;; 0.16161428 0.09552093 0.009707062; 0.015246317 -0.05086356 -0.10156621; 0.09224746 -0.15745741 -0.22876668;;; -0.09181874 -0.3204108 -0.14883812; -0.19851913 -0.015253798 0.08072419; -0.09112893 -0.015008952 0.46775427;;; â€¦ ;;; 0.11056245 -0.0435821 0.036314752; -0.06547583 -0.11904213 0.10716737; -0.07213809 -0.17660402 0.4032243;;; 0.04780201 0.041345984 -0.08005226; -0.0405121 -0.008105615 -0.021749947; -0.027794257 -0.070356466 -0.113246016;;; 0.1434623 0.24416183 0.06612582; 0.11707073 0.18852706 0.0918565; 0.1153921 0.13038169 0.07107783;;;; 0.096283264 0.022924708 0.19488849; -0.482501 0.038342465 0.18895954; -0.057514 0.015736349 0.037161864;;; -0.32380354 0.073853545 -0.07103055; -0.29975072 -0.23983386 -0.11476153; -0.43629107 -0.18671203 -0.07305557;;; 0.1196177 -0.13258472 -0.11620584; 0.03564006 0.115414634 -0.2777035; 0.15630159 -0.2568753 -0.20712957;;; â€¦ ;;; -0.028732048 0.113016926 -0.10299222; -0.19620886 0.10114032 -0.1420607; -0.13830073 0.03362876 -0.05606734;;; -0.04574185 -0.0139484005 -0.03677386; 0.09918358 -0.010560197 0.100250386; -0.0214711 0.04216716 -0.058087267;;; 0.17857444 0.2858399 0.2532417; 0.1649808 0.2951202 0.1336261; 0.30466485 0.25091422 0.1086166;;;; â€¦ ;;;; 0.2715202 -0.18116929 0.26235482; 0.29681742 0.2411111 0.1508174; -0.034519304 0.32325068 -0.015417469;;; -0.0899424 0.020180788 -0.056510657; 0.0009397509 -0.02124759 -0.013634795; -0.15775168 0.083124526 -0.112784736;;; 0.26711634 0.08289399 -0.025624277; 0.2732215 0.20596729 -0.06602824; -0.011005299 0.09813465 -0.05298669;;; â€¦ ;;; 0.2693473 -0.24294524 0.11661283; 0.14213233 -0.06677036 0.15215378; 0.025786517 -0.07832894 0.102152154;;; -0.026996534 0.11721461 -0.06633413; -0.01063641 0.0858939 0.14592253; -0.037614778 -0.0040320894 0.17147025;;; 0.20945257 0.16610134 0.27852717; 0.22080582 0.12872516 0.20200779; 0.20196487 0.14260685 0.10625953;;;; -0.006585136 0.1706344 0.12964687; -0.022020007 -0.29697993 -0.24739742; -0.035287894 0.21117489 0.32500088;;; -0.0022922994 0.15755661 -0.053557716; 0.047596306 0.16220404 0.020865515; 0.13179572 0.14619811 0.14686516;;; 0.25921294 -0.015064376 0.10085299; 0.01642098 0.08046403 0.13571718; -0.079959914 0.18401363 0.16520175;;; â€¦ ;;; 0.19277947 0.026342168 0.26743385; 0.1068452 -0.2069778 0.19086522; 0.014068703 -0.066518664 0.54632974;;; 0.09136185 0.15612543 0.12362238; 0.12224714 0.11533892 0.031859152; -0.008252751 0.13239135 0.015468051;;; -0.003446766 0.03331135 0.13941756; -0.10050402 0.008770146 0.11598564; -0.029603247 -0.061131857 0.054390326;;;; -0.5238692 -0.072025634 -0.33039266; -0.4024472 -0.2156034 -0.14642419; -0.11653956 -0.07602964 -0.013419213;;; 0.69656116 0.7689354 0.60095996; 0.6306031 0.76765376 0.59260046; 0.6362334 0.80885 0.69590724;;; 0.08209318 0.1345231 0.13449548; -0.008340398 0.06770548 0.107863575; -0.0826766 -0.09402867 0.2289785;;; â€¦ ;;; -0.18100432 -0.25673944 -0.1269177; -0.046445545 0.0137915285 0.12932405; -0.39386022 -0.17778601 -0.25871935;;; -0.012466985 0.071740404 0.06372072; 0.02383387 0.16489635 0.056488298; -0.04874223 0.06447248 0.13133617;;; 0.1998899 0.30242905 0.35685328; 0.24832453 0.30699703 0.33572054; 0.33320835 0.26473102 0.3709722], bias = [-0.00011713756;;; 0.00034370765;;; 0.00022186498;;; â€¦ ;;; -0.0014784194;;; -0.00010091571;;; -0.0002111298;;;;]), layer_4 = (scale = Float32[0.9116556, 0.94430524, 1.0475794, 0.68458754, 1.2092435, 0.9825204, 1.053736, 0.92289615, 0.90910447, 1.0481764  â€¦  0.8961099, 1.019379, 1.187289, 0.7795962, 1.4719079, 0.9005972, 0.92875457, 0.8369814, 0.71075296, 0.9282332], bias = Float32[-0.14651927, -0.22759326, -0.29696056, -0.8076904, 0.12809837, -0.5052222, -0.11489471, -0.37840775, -0.65037805, -1.0595227  â€¦  -0.2769471, 0.16083407, -0.13251953, -0.010122776, 0.08356841, -0.06756788, -0.16914721, -0.55064523, -0.24111602, -0.18206936]), layer_5 = (weight = [-0.27295586 0.03895171; -0.06134541 -0.269708;;; 0.25967804 0.088910624; 0.20099938 0.20324904;;; 0.042429715 0.20929243; -0.049416684 -0.08016512;;; â€¦ ;;; -0.48601982 -0.5401661; -0.40330508 0.15439282;;; 0.1748629 0.23332617; 0.343107 0.10071906;;; 0.17827533 0.22421658; 0.025274232 0.3037144;;;; -0.57762945 0.3087088; 0.11700531 -0.5032301;;; 0.5729755 0.19192249; 0.022407899 0.05926676;;; 0.22149822 0.23887679; 0.089977205 0.0017480323;;; â€¦ ;;; -0.23535876 -0.13809454; -0.026758023 -0.06690427;;; 0.48465607 0.4182933; -0.28437334 0.30854017;;; 0.18150769 0.2747838; -0.12928784 -0.34355542;;;; -0.30242535 0.03237508; -0.31653452 -0.38912404;;; -0.11436149 -0.2258899; 0.013699391 -0.04362415;;; 0.19963816 0.017762292; 0.054079983 -0.07975438;;; â€¦ ;;; 0.23214132 -0.42706388; 0.1422714 -0.31476316;;; -0.08028821 0.26439372; -0.03637836 -0.18721259;;; -0.11242016 -0.029029882; -0.10772546 0.44051427;;;; â€¦ ;;;; -0.18726227 -0.055735834; -0.2849094 -0.06815044;;; 0.16338177 0.01782867; -0.022695955 -0.0650187;;; -0.2377462 -0.040480573; 0.07638743 0.05177886;;; â€¦ ;;; 0.18267472 -0.10734419; 0.25284976 0.24721867;;; 0.2697514 0.069827475; -0.025701482 0.39609855;;; 0.15545996 -0.6297392; 0.17359409 0.11775403;;;; -0.24095353 0.10511358; 0.20854178 0.28341684;;; -0.13460907 0.04141566; 0.08393036 -0.022111518;;; -0.08226099 -0.021565512; 0.010685727 -0.17204039;;; â€¦ ;;; -0.1505647 -0.13276856; -0.03336625 -0.027003499;;; 0.050611407 -0.15057799; -0.1028596 -0.08873162;;; 0.21409601 0.00028231292; 0.27597427 0.06869985;;;; 0.37697262 0.15280193; 0.24914753 0.36695394;;; 0.04480045 0.061092928; 0.087681055 -0.011353772;;; -0.12333426 -0.22602046; -0.13967879 0.016726961;;; â€¦ ;;; -0.1259574 -0.1980854; -0.16628522 -0.053003687;;; 0.20838101 0.2677572; -0.082477264 0.14615634;;; 0.1707487 0.08429102; 0.09335278 0.0052728825], bias = [-0.017238798;;; -0.0011728316;;; 0.0038641973;;; â€¦ ;;; 0.0014065634;;; -0.02083312;;; -0.0030543192;;;;]), layer_6 = (scale = Float32[1.0587863, 1.2556139, 1.3144346, 1.2476578, 0.9622811, 1.358313, 1.2013321, 1.2387851, 0.995221, 1.1452729, 1.1342047, 0.9965826, 1.2558886, 1.2257116, 1.1875144, 1.3650825], bias = Float32[-0.041754045, -0.03039881, 0.026939984, 0.22103882, -0.7801735, -0.027759707, -0.3336802, -0.25156724, -0.23927723, 0.018774047, -0.56994116, -0.55163074, -0.21844383, 0.26468587, -0.4551946, 0.04514246])), l7 = (layer_1 = (weight = [-0.2090977 -0.084750906 -0.02323295; -0.036628254 -0.018364528 -0.002079387; 0.012071196 -0.124330476 -0.0622764;;; -0.44173425 0.06352252 -0.094153695; -0.43006572 0.1091018 0.018022548; -0.37577385 -0.039662685 -0.08665088;;; -0.18679723 -0.36081585 -0.7128983; -0.21959664 -0.14814095 -0.55966115; -0.11198876 -0.37197185 -0.6745687;;; â€¦ ;;; 0.0034801702 0.029820232 0.0018244904; -0.046149142 -0.07603915 0.0024379503; 0.06064871 -0.043030944 -0.08684766;;; 0.119705886 0.15660089 0.07270939; -0.0006149672 0.10479051 0.08254483; 0.09233137 0.093628205 0.09245821;;; 0.0003426577 0.07253802 0.14422409; 0.121744394 0.025544476 -0.003549421; 0.0066480706 0.098648906 -0.023604495;;;; 0.0026552284 -0.1288658 -0.06890153; 0.12540157 -0.15465747 0.14858122; -0.163307 -0.10904206 -0.23387896;;; 0.21200404 -0.29146463 -0.4732528; 0.17070389 -0.15323174 -0.25514904; 0.09392653 -0.039195586 -0.6557091;;; -0.27635047 -0.181213 -0.35569072; -0.22376698 -0.15679604 -0.5447544; -0.31453657 -0.25140065 -0.10827329;;; â€¦ ;;; 0.26573843 0.05259351 0.07937041; 0.04871775 0.13171433 0.06667171; 0.18639582 0.21695273 -0.025138011;;; -0.046006408 -0.05608037 0.05307854; 0.066781096 0.043083463 -0.07032779; -0.15395091 -0.052892942 -0.116236344;;; 0.075275905 -0.017100988 0.21327418; 0.013650127 0.093266726 0.0462151; 0.02212217 0.19911821 0.21975216;;;; -0.31125054 -0.31828856 -0.4415444; -0.15193085 0.11916885 -0.53541154; 0.18074813 0.077754244 -0.2664288;;; 0.11965178 0.17906873 0.12766409; -0.29069775 -0.32658893 0.089157596; -0.09560376 -0.36416617 -0.4177394;;; 0.21165605 0.3752469 0.5234762; 0.2336477 0.33050787 0.5499013; 0.11402254 0.18391144 0.12859122;;; â€¦ ;;; -0.10458489 -0.12928101 -0.2265293; 0.20131189 0.06244348 -0.20589595; 0.1819913 0.085847475 0.08748974;;; -0.12132008 -0.06715763 0.10161383; -0.14673302 -0.16729069 -0.10738545; -0.0066133295 -0.053190287 0.067081936;;; -0.024556227 0.10879474 0.017529365; 0.103799865 -0.04453708 0.09666409; -0.10672872 -0.16625577 -0.07640287;;;; â€¦ ;;;; -0.15054521 -0.35185775 -0.19535786; -0.26570424 -0.27662256 -0.3728316; -0.1498923 -0.3143437 -0.14806831;;; 0.022568665 0.20181921 0.2887915; 0.034026224 0.14701527 0.396024; 0.16140386 0.25747243 0.17841321;;; 0.36088097 0.47843647 0.65254855; 0.36210248 0.6914303 0.876686; -0.054622784 0.4983951 0.5226916;;; â€¦ ;;; -0.0014609171 -0.18084235 -0.00996596; -0.044750523 0.0077004833 0.14240964; -0.047706023 0.0566671 0.040589236;;; -0.16539207 -0.14201336 -0.011003707; -0.13776343 -0.09338274 0.043688603; -0.037827715 -0.17371552 0.03973326;;; -0.077673726 0.11953116 -0.0472009; -0.0044016824 -0.034397826 -0.07347345; -0.108719595 0.111322775 -0.1364605;;;; 0.37151423 0.37673438 0.3181595; 0.28377035 0.1824682 0.30675462; 0.4178452 -0.025890052 0.1427766;;; -0.025674768 0.27911654 0.012679712; -0.32608998 -0.007530427 0.4394368; -0.41001013 -0.049576927 -0.4467492;;; -0.19140312 -0.20292906 -0.106588066; 0.034800608 0.16419518 0.057669442; -0.28551888 -0.24810374 -0.2665926;;; â€¦ ;;; -0.03557732 -0.06114546 -0.29251897; -0.03321305 0.15661852 -0.17836158; 0.08407095 -0.052085366 -0.077094726;;; 0.07888915 -0.16319352 -0.13611041; 0.13772263 0.11635855 -0.0777005; 0.22482419 0.30532607 0.07511762;;; -0.17657238 0.008572399 -0.014324215; -0.04226897 -0.17197368 -0.07262281; -0.167177 -0.14204296 -0.24231042;;;; 0.08553121 -0.043337107 0.20346922; -0.36135173 -0.09639059 -0.23323992; 0.17243952 -0.014939384 0.22543679;;; -0.19630915 -0.0058171568 -0.0030570996; 0.037109584 0.15137297 -0.25360325; -0.17631395 -0.09436506 -0.10385255;;; -0.05990334 -0.044524226 -0.37825584; -0.00030071638 0.15641499 -0.11173224; 0.14707254 -0.0319143 0.007909909;;; â€¦ ;;; -0.049864765 0.06352734 0.069168985; -0.06816141 0.09636566 -0.043025564; 0.06337694 0.06689426 -0.0055934377;;; 0.0053071077 -0.04785391 -0.047545273; -0.012677679 -0.028134918 -0.0837709; -0.049538203 -0.08786071 0.016755255;;; -0.17262022 -0.034290686 -0.13119951; -0.08982034 -0.09328084 0.0070700077; -0.11064826 -0.17115842 -0.09394044], bias = [-0.0017169056;;; -0.0014985487;;; 2.1943944f-5;;; â€¦ ;;; 0.0008714339;;; 0.00050444884;;; -0.0046624322;;;;]), layer_2 = (scale = Float32[0.91780305, 1.0447929, 0.75343907, 0.7190609, 0.85264874, 0.81803274, 0.86143357, 1.0028757, 0.8965801, 0.8934157, 1.0476366, 0.84273714, 0.7911985, 1.2520603, 0.9511295, 1.2581345], bias = Float32[-0.041647512, 0.033808928, 0.4083873, -0.38491502, -0.4138728, -0.11679481, -1.0142856, -0.11538869, 0.02889607, -0.7317613, 0.09349216, 0.41594085, 0.1935368, 0.2584225, -0.12827739, 0.33345625]), layer_3 = (weight = [-0.17572697 -0.025719846 -0.0014674623; -0.11696026 0.08920115 0.045189325; -0.11843731 0.1413098 0.07112547;;; -0.15445288 -0.44845662 0.004441256; -0.118951246 -0.33622053 0.12897359; -0.061545074 -0.32600102 0.10386537;;; -0.044855606 0.16781263 -0.112393886; -0.2528735 0.035937898 -0.378539; -0.11315177 0.05903311 -0.18773834;;; â€¦ ;;; 0.11107888 -0.018999524 -0.058575947; -0.055474635 -0.13613564 0.07079084; -0.07647218 -0.12453242 0.112611584;;; 0.13133982 0.111755736 0.061674412; 0.26944354 0.18476452 0.41652185; -0.16240877 0.29876035 0.2581173;;; -0.38875213 -0.14852251 -0.09793739; -0.47107673 -0.15106112 -0.17811123; -0.35828272 -0.2875694 -0.045658734;;;; 0.031597033 0.08677054 -0.31307262; 0.106516674 -0.045705177 -0.11572761; 0.16143884 0.0197709 -0.18732667;;; 0.20842417 0.1559019 -0.48067093; 0.13247548 0.45855564 -0.3886203; 0.1808987 0.11393308 -0.31385872;;; -0.0060251923 -0.24691294 0.23660333; -0.0600747 -0.31116432 -0.24887797; -0.25127542 -0.28252506 0.02792729;;; â€¦ ;;; -0.07752718 0.06719948 -0.10776333; -0.103252284 -0.13770384 -0.080098055; 0.071892224 -0.024211729 -0.11126177;;; 0.104948506 0.47740665 -0.07172417; 0.8463064 -0.02429069 0.16265152; 0.14194207 0.6740024 -0.062112156;;; 0.07123666 -0.32176098 -0.25984383; -0.18358034 -0.25982025 -0.30572864; 0.02100266 -0.33739358 -0.09208897;;;; 0.10225007 0.34326485 0.18514958; 0.44012627 0.34706473 0.28503436; 0.38756722 0.22009271 0.1102302;;; -0.06927738 -0.26233214 0.022174397; 0.05877169 -0.25604674 0.105045125; -0.08819481 -0.19675596 0.02513055;;; 0.12545423 -0.0015911671 0.13368264; -0.21968281 -0.43755847 -0.27314985; -0.4015274 -0.34636354 -0.3038421;;; â€¦ ;;; 0.04638704 0.19923076 0.13101947; 0.1254212 0.034217082 0.18084502; 0.24554808 0.016666614 0.06965209;;; -0.07091159 -0.029475864 -0.048955992; 0.11104996 0.12973961 0.26241487; 0.014182495 -0.034160603 0.11572272;;; 0.10603893 0.19629638 0.22325724; -0.01451616 0.10764326 0.018528778; 0.025933145 0.05422468 0.17067836;;;; â€¦ ;;;; -0.09026317 0.0022911169 -0.13094126; 0.1057824 -0.07412353 -0.07945265; -0.0053026327 -0.0469427 -0.2840532;;; -0.15682225 0.22755092 -0.34778595; -0.23772104 0.24330266 -0.2972973; -0.1580969 0.17582504 -0.227701;;; 0.25208795 -0.13256721 -0.08470976; -0.15170138 -0.4237988 0.27125168; 0.055464655 -0.23682438 -0.1471552;;; â€¦ ;;; 0.12838852 0.08177192 -0.123313606; 0.05667593 0.0334139 -0.06644294; 0.020075774 -0.11020865 -0.116189376;;; 0.21253504 -0.20690916 -0.04568141; 0.07902865 0.6841511 -0.32772946; 0.54211605 -0.09715659 -0.009138437;;; -0.26425755 -0.031996362 -0.40889984; -0.02543744 -0.3541229 -0.14462623; -0.13479589 -0.027538056 -0.2781592;;;; -0.21536015 -0.14008273 -0.17257516; -0.4213912 -0.390405 -0.42817473; -0.42484134 -0.44044113 -0.22927333;;; -0.117761485 -0.39088324 -0.2809122; -0.43130597 -0.46422648 -0.40379155; -0.32893547 -0.3914463 -0.09373729;;; 0.10994702 0.20149435 0.095439844; 0.17943376 0.28821486 0.14282872; 0.07376014 0.086491965 -0.020947577;;; â€¦ ;;; 0.22612935 0.33173403 0.32507867; 0.29449528 0.31213555 0.21024366; 0.23838237 0.33713722 0.43084326;;; -0.069085956 -0.057412907 -0.07544838; -0.0700198 -0.063129224 -0.1946061; -0.004710763 -0.03207337 -0.14729813;;; -0.15804173 -0.076254636 -0.14899409; -0.17059067 0.012081705 -0.04169362; -0.15645692 -0.14622962 -0.2128164;;;; -0.3239731 0.23439822 0.21027653; -0.30566183 0.122937016 0.16751851; -0.1372519 0.2740841 0.12285938;;; -0.23572844 -0.25904018 -0.032851133; -0.11127945 -0.3384635 -0.13079913; -0.04698961 -0.49496573 0.09256464;;; -0.0075562363 0.16216856 -0.2105411; -0.17360754 0.03856652 -0.38360953; -0.24305975 0.269384 -0.10299866;;; â€¦ ;;; 0.04604805 0.004335559 0.01828811; 0.19322468 -0.01255265 0.1686922; 0.11164311 -0.06640673 0.014153448;;; 0.058894277 -0.061167017 0.14753173; -0.11276869 0.033878412 0.2222557; -0.36237216 -0.25026637 0.3897529;;; -0.1740113 -0.12549785 -0.02916515; -0.41891047 -0.112647966 -0.14234538; -0.23268507 -0.20669112 0.053704504], bias = [-0.62066615;;; -0.73914313;;; -0.8515804;;; â€¦ ;;; -0.70718366;;; 0.7666363;;; -0.4270995;;;;]), layer_4 = (scale = Float32[0.8271365, 0.85117596, 1.0489444, 1.9182203, 2.2053828, 0.8589097, 1.7210823, 2.8236625, 1.993023, 2.2895415, 0.96436363, 0.95208716, 0.784216, 0.71176076, 1.4028443, 0.8576321], bias = Float32[-0.60424644, -0.55171794, -0.23520291, 0.6795408, 1.3066454, -0.5940027, 0.5967772, 1.4407701, 0.56893075, 0.5571545, -0.2650764, -0.7192725, -0.5639353, -0.5347722, 1.1176457, -0.60436577]), layer_5 = (weight = [0.33798873;;; 0.36089236;;; 0.20001608;;; â€¦ ;;; 0.3994445;;; -1.0924011;;; 0.257982;;;;], bias = [-0.62422943;;;;]), layer_6 = NamedTuple())), (l1 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-1.5754029, 2.2304332, -1.6438724, 2.1291182, 1.5642703, -4.459931, -1.4128605, 2.5394652, 0.08220151, 1.3944055, -2.2787423, 2.0576603, -1.6820405, 1.7552352, 2.6440036, -2.1005633], running_var = Float32[2.0897603, 2.1679688, 0.8702852, 0.7614294, 0.4340817, 9.882541, 1.1594844, 3.4043326, 0.6023198, 1.5117741, 2.6206417, 2.2926908, 1.1610765, 1.3491561, 1.8846543, 1.7653261], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-0.26003373, 0.75799036, 1.0921711, -2.363193, -2.6438394, 0.91217846, -11.351362, 1.798894, -0.120637566, -1.7964456, -2.17908, 0.97645134, -0.6907514, -0.715033, 0.84746337, 1.8764355], running_var = Float32[0.86277986, 2.34481, 0.5159975, 3.5334196, 2.3837838, 4.866938, 86.0603, 4.1092596, 0.39124367, 4.560316, 4.7320285, 2.6757293, 1.6857156, 1.596671, 2.1655583, 3.9297612], training = Val{true}())), l2 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[1.5154504, -1.9864492, -0.05900887, -1.8279338, -0.45725176, -1.7132331, -1.5774999, -1.6919795, -1.8124511, -1.2015193  â€¦  -1.8943713, 0.8022662, -0.28530863, -5.601649, 0.1532071, 0.05182159, -0.6386317, -2.187148, -0.25315025, -3.1455092], running_var = Float32[9.6278515, 3.6874192, 1.7821552, 7.6003714, 7.908434, 4.0281296, 2.8905642, 5.1883016, 3.7529757, 4.8467627  â€¦  6.0086713, 2.4655778, 2.5662675, 23.759802, 5.0576425, 5.8578525, 3.6530638, 11.6891575, 2.382889, 6.4362507], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-0.38670686, -3.4821405, 0.8948534, -7.470035, -0.3325103, -0.80308604, -0.8082723, -1.1250818, -4.366359, 0.020487003  â€¦  -2.1897147, -4.647575, -0.8263539, -1.5992923, 5.158841, 0.285445, -5.0593944, 0.25321457, -1.6346327, -0.30238426], running_var = Float32[13.25464, 13.936933, 10.781684, 31.032028, 5.566, 7.282704, 17.183945, 20.748896, 15.620119, 5.12627  â€¦  8.335145, 17.327988, 8.0336, 6.9813347, 6.6565514, 3.8190966, 32.76575, 12.917389, 11.410164, 7.671706], training = Val{true}())), l3 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[-4.5070324, -3.6086025, -3.3104753, -1.2870357, -3.6962695, 2.478482, -3.0897262, -2.072947, -0.92573875, -3.5626233  â€¦  -0.8161328, -2.3371005, -3.2450821, -0.86261946, -0.29698133, -1.2981201, -9.703284, -1.4329699, -1.0966451, 3.011059], running_var = Float32[15.987832, 10.599855, 12.930322, 35.928932, 11.445602, 10.5548725, 28.689053, 15.338634, 6.3653245, 8.0531845  â€¦  8.627871, 9.290679, 22.849585, 7.559071, 6.0918703, 6.8685684, 54.55773, 8.032581, 11.159955, 12.744537], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-3.1847692, -4.411163, 0.13701688, -2.1381001, 4.199629, -4.199776, -3.7285619, 3.5002022, -5.1926847, 3.2604406  â€¦  -2.4859455, -3.3416178, -2.9819596, 0.2907448, -1.8212411, -3.7625725, -1.4776698, -2.7399633, -0.7259112, -4.45209], running_var = Float32[27.021809, 29.903276, 18.154123, 16.25652, 39.594326, 23.51737, 24.635033, 29.815945, 15.189225, 18.707788  â€¦  15.680192, 23.456427, 19.503086, 26.018862, 16.511292, 18.987051, 19.733242, 17.137589, 11.813095, 20.573347], training = Val{true}())), l4 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[-4.6018972, -6.4119287, -0.6601016, -3.972513, 0.73298705, 0.32276353, -2.5916796, -0.9283724, 8.84205, -3.5314968  â€¦  -2.7598455, -4.5599008, -10.949865, -3.4567327, 0.4007349, -2.0541868, -6.2357907, -6.5835023, -4.220362, -1.7432785], running_var = Float32[12.170371, 16.407602, 11.219261, 10.239388, 9.127764, 7.601739, 11.995673, 9.809753, 30.651272, 10.435767  â€¦  10.799029, 9.061483, 30.719759, 13.132886, 8.461739, 9.636921, 19.636986, 14.243659, 16.243866, 13.581226], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-1.2959343, -1.2909658, 0.16663474, -4.2599816, -2.3000445, 3.570889, -2.4836326, -2.0251374, -1.7749065, -3.108264  â€¦  -0.39329255, -2.1871207, -2.068814, -3.9509804, -2.0536783, -0.43730664, -3.0139768, -0.100325495, -3.9822783, -4.7487917], running_var = Float32[12.871855, 10.011332, 11.883709, 27.74107, 9.609573, 11.742779, 9.470422, 9.121495, 9.6405325, 21.981926  â€¦  10.593099, 9.594905, 17.374401, 15.45239, 15.388648, 18.774017, 13.540326, 10.264322, 9.410594, 41.94588], training = Val{true}()), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[-0.8075587, -0.35784167, -0.3256879, 0.8113212, -1.0499439, -0.4906547, -1.0537039, -0.9429849, -1.6440116, -0.8446082  â€¦  -0.98818624, -1.2170913, -1.0054722, -1.1656944, -0.8214088, -0.8918874, -0.46898264, -1.0942973, -1.3845578, -1.2448634], running_var = Float32[1.7556516, 2.4620602, 1.573752, 1.8793017, 1.8735964, 1.3580947, 1.2465026, 1.463065, 2.7665656, 1.7498118  â€¦  1.5953432, 1.7570844, 1.9325224, 1.6127695, 1.5213866, 2.4579513, 1.8512408, 1.520702, 1.6887246, 2.6519449], training = Val{true}())), l5 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-1.7049261, -12.799857, -10.7579775, -2.3393955, -5.3553457, -11.133736, -11.088706, -0.24853665, 2.0720465, -6.1234565  â€¦  -3.1254816, -24.402235, -8.899295, -11.349597, -12.9706545, -11.474875, -8.14318, -14.117596, -3.7789216, -7.6217227], running_var = Float32[31.714796, 45.48835, 17.298887, 14.214306, 15.716167, 45.40012, 48.47154, 46.425404, 19.051628, 13.661215  â€¦  36.58498, 134.77783, 24.090263, 84.66989, 24.387972, 29.898108, 19.604326, 96.977005, 21.688875, 18.152905], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-5.969445, 11.112707, -2.297911, 0.2532852, -6.8239374, -3.639218, -1.8662655, -1.6908175, -2.38462, -2.9967182  â€¦  5.9437532, -10.00198, 3.062419, -6.4528995, -3.8245046, -2.4728782, -8.332784, -2.7674334, -3.5066917, -9.648895], running_var = Float32[19.060806, 15.844644, 16.522848, 10.065914, 25.591347, 6.1295753, 6.6551595, 16.034666, 5.864401, 16.201351  â€¦  27.601461, 34.51728, 9.647924, 14.70416, 35.08426, 7.278095, 35.32991, 7.394416, 13.488609, 20.409946], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = (running_mean = Float32[-1.5234184, -1.2186186, -0.02490592, -0.78027123, -0.34944618, -0.7902519, -0.95135164, -0.6897209, 0.2480168, -1.0789561  â€¦  -0.91608727, -0.14998516, -0.2579602, -0.36708957, 0.11641896, -0.22830302, -0.27364036, -1.0914941, -0.3476661, -0.90150803], running_var = Float32[1.728971, 1.2626207, 2.2535167, 1.2932254, 0.9994264, 0.52282625, 1.4398298, 0.77620417, 0.9257943, 1.4324213  â€¦  1.0207062, 0.73044306, 1.0497355, 2.0980537, 0.8629615, 0.64879704, 0.9555804, 1.6620896, 1.3908074, 1.1800174], training = Val{true}())), l6 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.45398077, 3.1040728, -4.1734366, -9.457641, -4.385779, 0.29065302, 11.094552, -11.427861, -11.377348, -1.7000929  â€¦  -3.0779853, -10.214769, -12.478619, -4.480768, -7.092387, -8.477194, -0.53878605, -3.0072377, -10.478661, -13.429768], running_var = Float32[24.1616, 37.0673, 21.684593, 24.465296, 11.908242, 33.754585, 27.94479, 67.092, 30.679657, 14.620921  â€¦  15.834817, 76.85379, 97.55973, 31.168755, 19.895134, 19.94756, 18.443316, 29.511322, 71.37834, 79.24699], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-7.350101, -1.3057345, 0.09133152, 0.69866395, 0.20496458, 2.4354463, -3.1037986, -1.6975197, 2.4328735, 5.2691216  â€¦  -4.399921, -6.570023, 0.09587215, -0.15653865, -0.42224, -0.32776442, -2.9438632, 0.8235666, 1.9659805, 3.46391], running_var = Float32[49.17146, 9.221922, 9.571911, 4.2467732, 9.096215, 11.094487, 19.29322, 19.021053, 8.27593, 4.2129154  â€¦  17.527779, 73.72455, 11.014356, 9.312718, 16.919437, 14.305601, 21.291338, 8.370148, 5.4509015, 10.0234995], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = (running_mean = Float32[-0.88177675, 0.74388593, 0.79187936, -0.5618399, 0.22302414, -0.041371685, 0.6301633, 0.17459732, -0.5959811, -0.91441065, 0.48147509, -0.11453826, 0.79867584, -0.9572144, 0.6485394, 0.83113277], running_var = Float32[0.83086854, 0.5760305, 0.5823655, 1.2953343, 0.46762547, 0.75991756, 0.5340705, 0.71516967, 0.575326, 0.8368994, 0.73958755, 0.5081083, 0.38439253, 0.8508277, 0.64039004, 0.6822942], training = Val{true}())), l7 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.9361402, -1.8351213, -0.6547662, -6.233516, 3.905557, 3.791931, 2.694274, 3.806114, 0.86151266, 4.210085, 4.489805, -6.0706887, 2.493828, -2.8929956, 0.9997623, 0.21269886], running_var = Float32[57.14985, 42.12395, 13.565391, 37.170547, 14.667648, 17.328228, 22.091482, 55.127613, 42.91621, 20.877857, 40.242283, 33.235554, 32.552547, 98.26823, 9.651412, 48.53098], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-2.4264116, -1.5976502, -0.7153492, -0.7808589, -0.7240819, -3.690629, -0.6527047, -0.41899863, 0.115406215, 3.0912266, -0.22297582, -3.053255, -2.1707454, -2.9569552, -0.06372799, -3.8138742], running_var = Float32[5.8451667, 3.2155938, 15.752311, 63.16103, 76.92424, 6.4162664, 51.739735, 62.66319, 43.013035, 38.877594, 14.477288, 11.7569685, 5.209212, 4.4188404, 89.966095, 5.273845], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = NamedTuple())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@load path_to_saved_model ps_save st_save\n",
    "ps, st = ps_save |> dev, st_save |> dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"./dataset/inter-observer/50_Sohrab_Fati/output\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root = raw\"./dataset/inter-observer/50_Sohrab_Fati/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_image (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function prepare_image(curr_dir, sid, f_name)\n",
    "    dcm_path = joinpath(curr_dir, sid, \"source\", f_name * \".dcm\")\n",
    "    breast_mask_path = joinpath(curr_dir, sid, \"source\", f_name * \".mask.png\")\n",
    "    # ground_truth_mask_path = joinpath(curr_dir, sid, f_name * \".png\")\n",
    "\n",
    "    # ground_truth_mask = Float32.(Images.load(ground_truth_mask_path))\n",
    "    breast_mask = Float32.(Images.load(breast_mask_path))\n",
    "    dcm_data = dcm_parse(dcm_path)\n",
    "    pixel_size = dcm_data[(0x0018, 0x1164)]\n",
    "    is_reversed = uppercase(dcm_data[(0x2050, 0x0020)]) == \"INVERSE\"\n",
    "    img = Float32.(dcm_data[(0x7fe0, 0x0010)])\n",
    "    original_size = size(img)\n",
    "    ground_truth_mask = zeros(Float32, original_size)\n",
    "    # resize image based on pixel length\n",
    "    img, breast_mask, ground_truth_mask, new_size = resize_dicom_image(img, breast_mask, ground_truth_mask, pixel_size)\n",
    "    # normalize image and correct color\n",
    "    img = normalize_img(img; mask=breast_mask, invert=is_reversed)\n",
    "    # crop to breast only\n",
    "    img_cropped, ground_truth_mask_cropped, coords = crop_to_bounding_box(breast_mask, img, ground_truth_mask)\n",
    "    # I CAN THROW THE 'img_cropped' thing into the finished BAC model.\n",
    "    # save resize info to local\n",
    "    @save joinpath(curr_dir, sid, \"source\", f_name * \"_resize_info.jld2\") original_size new_size coords\n",
    "    # check size\n",
    "    x, y = size(img_cropped)\n",
    "    # if y % 32 != 0\n",
    "    #     x_org, y_org = size(img)\n",
    "    #     println(i, \"\\t\", ct+1)\n",
    "    #     println(\"($x_org, $y_org)\")\n",
    "    #     println(\"($x, $y)\\n\")\n",
    "    # end\n",
    "    @assert x % 32 == 0\n",
    "    @assert y % 32 == 0\n",
    "\n",
    "    #save\n",
    "    @save joinpath(curr_dir, sid, \"source\", f_name * \"_cropped.jld2\") img_cropped\n",
    "    # Images.save(joinpath(out_dir, f_name*\".png\"), Gray.(round.(ground_truth_mask_cropped)))\n",
    "    GC.gc(true)\n",
    "\n",
    "    return img_cropped\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_model_to_directory (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apply_model_to_directory(main_directory::String)\n",
    "  # Initialize a counter to keep track of the number of files processed\n",
    "  processed_count = 0\n",
    "  max_files = 4\n",
    "\n",
    "  # Iterate over each subdirectory in the main directory\n",
    "  for subdirectory in readdir(main_directory, join=true)\n",
    "    # Process only directories starting with \"SID\"\n",
    "    if isdir(subdirectory) && occursin(r\"^SID\", basename(subdirectory))\n",
    "      source_dir = joinpath(subdirectory, \"source\")\n",
    "\n",
    "      # Process each file in the 'source' subdirectory\n",
    "      for file in readdir(source_dir, join=true)\n",
    "\n",
    "        # if processed_count >= max_files\n",
    "        #   break\n",
    "        # end\n",
    "\n",
    "        file_name, ext = splitext(file)\n",
    "\n",
    "        # Check if the file is a DICOM file\n",
    "        if isfile(file) && occursin(r\".*\\.dcm$\", file)\n",
    "          # Load and prepare the image\n",
    "          x = prepare_image(main_directory, basename(subdirectory), basename(file_name))\n",
    "          x = Float32.(reshape(x, size(x)..., 1, 1))\n",
    "\n",
    "          x = x |> dev\n",
    "\n",
    "          # Apply the model\n",
    "          Å·, _ = Lux.apply(model_to_use, x, ps, st)\n",
    "          Å· = round.(Å· |> cpu)[:, :, 1, 1]\n",
    "\n",
    "          # Load resize information and apply size conversion\n",
    "          resize_path = joinpath(source_dir, basename(file_name) * \"_resize_info.jld2\")\n",
    "          @load resize_path new_size coords original_size\n",
    "\n",
    "          Å· = convert_to_og_size(Å·, original_size, new_size, coords)\n",
    "\n",
    "          # Save the prediction and denoise\n",
    "          output_dir = joinpath(subdirectory, \"predict\")\n",
    "          output_path = joinpath(output_dir, basename(file_name) * \"_predict_original.png\")\n",
    "          save(output_path, Gray.(Å·))\n",
    "\n",
    "          drop_area_path = joinpath(output_dir, basename(file_name) * \"_predict.png\")\n",
    "\n",
    "          # Drop small areas\n",
    "          drop_small_areas(output_path, drop_area_path, 800)\n",
    "\n",
    "          # Denoise the image\n",
    "          denoise(drop_area_path, drop_area_path, (5, 5))\n",
    "\n",
    "          # Increment the processed file count\n",
    "          processed_count += 1\n",
    "        end\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convert_to_og_size (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function convert_to_og_size(Å·_cropped, original_size, new_size, coords)\n",
    "\n",
    "  # convert it back to the original size\n",
    "  Å· = zeros(Float32, (new_size...))\n",
    "  a, b, c, d = coords\n",
    "  Å·[a:b, c:d] = Å·_cropped\n",
    "  Å· = round.(imresize(Å·, original_size))\n",
    "\n",
    "  return Å·\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denoise (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function denoise(input, output, filter_size)\n",
    "  img = load(input)\n",
    "  filtered_img = mapwindow(median, img, filter_size)\n",
    "  save(output, filtered_img)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drop_small_areas (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function drop_small_areas(input, output, threshold_area=1000)\n",
    "    img = load(input)\n",
    "    gray_img = Gray.(img)\n",
    "    binary_img = gray_img .> 0.5\n",
    "    labels = label_components(binary_img)\n",
    "\n",
    "    areas = Dict{Int,Int}()\n",
    "\n",
    "    for i in eachindex(labels)\n",
    "        label = labels[i]\n",
    "        if label != 0\n",
    "            if haskey(areas, label)\n",
    "                areas[label] += 1\n",
    "            else\n",
    "                areas[label] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    output_img = similar(binary_img)\n",
    "\n",
    "    for i in eachindex(labels)\n",
    "        label = labels[i]\n",
    "        if label != 0 && areas[label] > threshold_area\n",
    "            output_img[i] = true\n",
    "        else\n",
    "            output_img[i] = false\n",
    "        end\n",
    "    end\n",
    "\n",
    "    save(output, output_img)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_to_directory(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dice_loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function dice_loss(y_true, y_pred)::Float32\n",
    "\n",
    "  if size(y_true) != size(y_pred)\n",
    "    error(\"Input images must have the same dimensions\")\n",
    "  end\n",
    "\n",
    "  y_true = Float32.(Gray.(y_true) .> 0.5)\n",
    "  y_pred = Float32.(Gray.(y_pred) .> 0.5)\n",
    "\n",
    "  y_true_flat = vec(y_true)\n",
    "  y_pred_flat = vec(y_pred)\n",
    "\n",
    "  intersection = sum(y_true_flat .* y_pred_flat)\n",
    "  true_sum = sum(y_true_flat)\n",
    "  pred_sum = sum(y_pred_flat)\n",
    "\n",
    "  dice_coefficient = 2 * intersection / (true_sum + pred_sum)\n",
    "  return 1 - dice_coefficient\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function score(main_directory)\n",
    "  results = DataFrame(sid=String[], image_id=String[], fati_vs_predict=String[], sohrab_vs_predict=String[])\n",
    "\n",
    "  # Initialize a counter to keep track of the number of files processed\n",
    "  processed_count = 0\n",
    "  max_files = 4\n",
    "\n",
    "  # Iterate over each subdirectory in the main directory\n",
    "  for subdirectory in readdir(main_directory, join=true)\n",
    "    SID = split(subdirectory, '/')[end]\n",
    "    # Process only directories starting with \"SID\"\n",
    "    if isdir(subdirectory) && occursin(r\"^SID\", basename(subdirectory))\n",
    "      predict_dir = joinpath(subdirectory, \"predict\")\n",
    "      fati_dir = joinpath(subdirectory, \"fati\")\n",
    "      sohrab_dir = joinpath(subdirectory, \"sohrab\")\n",
    "\n",
    "      # if processed_count >= max_files\n",
    "      #   break\n",
    "      # end\n",
    "\n",
    "      for file_predict in readdir(predict_dir, join=true)\n",
    "\n",
    "        file_name, ext = splitext(basename(file_predict))\n",
    "        sid = split(file_name, '_')[1]\n",
    "\n",
    "        # Check if the file is a DICOM file\n",
    "        if isfile(file_predict) && occursin(r\"._predict.png\", file_predict)\n",
    "\n",
    "          file_path_predict = joinpath(predict_dir, sid * \"_predict.png\")\n",
    "          file_path_a = joinpath(fati_dir, sid * \".png\")\n",
    "          file_path_b = joinpath(sohrab_dir, sid * \".png\")\n",
    "\n",
    "\n",
    "          if isfile(file_path_a) && isfile(file_path_b)\n",
    "            img_predict = load(file_path_predict)\n",
    "            img_a = load(file_path_a)\n",
    "            img_b = load(file_path_b)\n",
    "\n",
    "            fati_vs_predict = dice_loss(img_a, img_predict)\n",
    "            sohrab_vs_predict = dice_loss(img_b, img_predict)\n",
    "\n",
    "            push!(results, (SID, sid, @sprintf(\"%.2f\", fati_vs_predict), @sprintf(\"%.2f\", sohrab_vs_predict)))\n",
    "\n",
    "          end\n",
    "\n",
    "          # Increment the processed file count\n",
    "          processed_count += 1\n",
    "\n",
    "        end\n",
    "      end\n",
    "\n",
    "    end\n",
    "  end\n",
    "\n",
    "  CSV.write(\"results.csv\", results)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"results.csv\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_BAC_mass_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
